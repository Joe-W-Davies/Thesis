\chapter{Signal and background modelling}
\label{chap:sigbkg}
%TODO add the pre-firing here somewhere?

\section{Introduction}

The final results of this analysis are extracted by performing a maximum likelihood fit 
of the signal and and background models to the diphoton invariant mass distribution observed in data.
Models of the signal and background \mgg distributions are therefore required as inputs to the fit.
The signal model is derived from simulation, 
with a model constructed for each particle level stage 1 bin in each reconstructed analysis category.
Both the shape and normalisation of the model are parameterised as a function of \mH.
The data-driven background model considers a range of different functional forms to 
represent the smoothly falling background spectrum, 
following the approach described in Ref.~\cite{Envelope}.
The construction of both the signal and background models is described in detail in this chapter.
In addition, the treatment of systematatic uncertainties affecting the two models is discussed.

\section{Signal modelling}

The signal model is a parametric function of \mH which describes the shape of the \mgg distribution, 
together with the expected normalisation of this shape.
An independent model is constructed for each stage 1 bin in each analysis category
Additionally, since the \mgg shape depends on whether the right vertex (RV) 
or wrong vertex (WV) has been chosen, the model for each of these cases is constructed separately.

Each model consists of a sum of up to five Gaussian functions.
The number of Gaussian functions required depends on the shape of the \mgg spectrum 
and the available MC statistics.
Alternative functional parameterisations have been studied, 
such as the sum of a Gaussian function to represent the core of the \mgg distribution 
and an exponential to model each tail \cite{LouieThesis}.
This alternative parameterisation yields very similar signal models to the sum of Gaussian functions, 
and the final results are unaffected.

The parameters of the Gaussian functions are determined by performing a fit 
to the simulated \mgg distribution for each model.
In order to account for the fact that the mass of the Higgs boson is not known exactly, 
the model constructed is a continuous parametric function of \mH.
The dependence on \mH is determined by simultaneously fitting events simulated with 
three values of \mH: 120, 125, and \SI{130}{GeV}.
Each parameter of each Gaussian function is represented as a linear function of \mH; 
the shape of each model then consists of $2\left(3N_{\textrm{Gaus}}-1\right)$ parameters, 
where $N_{\textrm{Gaus}}$ is the chosen number of Gaussian functions.
The values of these parameters are all established by the simultaneous fit across mass points.
An example of the evolution of signal model shapes as a function of \mH, 
for the ggH 0J bin the 0J Tag 0 category, is shown in Figure~\ref{fig:sigbkg_interp}

%TODO replace the unnecssary RECO in these plots
\begin{figure}[hptb]
\centering
\includegraphics[width=0.49\textwidth]{Figures/SigBkg/GG2H_0J_RECO_0J_Tag0_interp_2016.pdf}
\includegraphics[width=0.49\textwidth]{Figures/SigBkg/GG2H_0J_RECO_0J_Tag0_interp_2017.pdf}
\caption{
  The evolution of the parametrised signal model shape as a function of \mH.
  The plots show the signal model for the ggH 0J bin in the 0J Tag 0.
  The left plot shows 2016 simulation, with 2017 simulation on the right.
}
\label{fig:sigbkg_interp}
\end{figure}

In some cases, there is an insufficient number of simulated events available 
to accurately model the shape for a given stage 1 bin, 
analysis category and vertex scenario combination. %threshold on nEvents set at 200
The shape is then replaced by that of the stage 1 bin 
which has the highest expected yield in the category under consideration.
This replacement procedure is motivated by the fact that events subject to the same selection
tend to have similar values of the diphoton mass resolution.

After each model has been constructed, the shapes from the RV and WV scenarios are combined.
The fraction of events in which the correct vertex is chosen 
is also described by a linear function of \mH; 
once determined, this is used to assign the correct normalisations for the RV and WV models.
An example of the signal model for the ggH 0J bin in the 0J Tag 0 category, 
after the models for the RV and WV scenarios have been summed,
is shown in Figure~\ref{fig:sigbkg_proccat}.

\begin{figure}[hptb]
\centering
\includegraphics[width=0.49\textwidth]{Figures/SigBkg/GG2H_0J_RECO_0J_Tag0_2016.pdf}
\includegraphics[width=0.49\textwidth]{Figures/SigBkg/GG2H_0J_RECO_0J_Tag0_2017.pdf}
\caption{
  The parametrised signal shape for the ggH 0J bin in the 0J Tag 0 category, 
  after the models for the RV and WV scenarios have been summed.
  The open squares represent weighted simulation events and the blue line the
  corresponding model. Also shown is the \seff value (half the width of the narrowest interval
  containing 68.3\% of the invariant mass distribution) and the full width at half of the maximum
  (FWHM). The left plot shows 2016 simulation, with 2017 simulation on the right
}
\label{fig:sigbkg_proccat}
\end{figure}

For each category, 
the models corresponding to the contributions from each stage 1 bin are then summed.
To normalise the contribution from each stage 1 bin correctly, 
the total number of expected events for each stage 0 process is obtained 
using the cross-sections and \Hgg branching ratio from Ref.~\cite{YR4} 
\footnote{The exception is ggH, 
for which the latest calculations at N3LO are used \cite{Anastasiou2015,Anastasiou2016}.},
and the measured intergrated luminosity in data.
The fraction of each stage 1 bin is then taken from the simulated events for each stage 0 process.
Finally, the product of the detector efficiency and analysis acceptance 
is modelled as a linear function of \mH, 
determined by the ratio of the total number of expected events 
to the number of events entering each analysis category.
Together these allow the total signal model for each category to be computed.
An example of the signal model for the 0J Tag 0 category, 
after the models for all the stage 1 bins have been summed, 
is shown in Figure~\ref{fig:sigbkg_cat}.

\begin{figure}[hptb]
\centering
\includegraphics[width=0.49\textwidth]{Figures/SigBkg/RECO_0J_Tag0_2016.pdf}
\includegraphics[width=0.49\textwidth]{Figures/SigBkg/RECO_0J_Tag0_2017.pdf}
\caption{
  The parametrised signal shape for the 0J Tag 0 category, 
  after the models for all the stage 1 bins have been summed.
  The open squares represent weighted simulation events and the blue line the
  corresponding model. Also shown is the \seff value (half the width of the narrowest interval
  containing 68.3\% of the invariant mass distribution) and the full width at half of the maximum
  (FWHM). The left plot shows 2016 simulation, with 2017 simulation on the right.
}
\label{fig:sigbkg_cat}
\end{figure}

%TODO insert sum of cats

\section{Background modelling}

The background model represents the smoothly falling spectrum 
in the \mgg distribution that results from processes other than Higgs boson production.
The shape of this falling distribution is not known \textit{a priori};
therefore different functional forms must be considered when constructing the model.
Each choice of function results in a different number of estimated events 
under the signal peak produced by the Higgs boson, 
and hence affects the measured value of parameters representing the size of the signal contribution.
The uncertainty in the measurement associated with this choice must be accounted for 
in the final results.
In this analysis, the discrete profiling method is used, as first described in Ref.~\cite{Envelope}.
The model is constructed independently for each analysis category.

The discrete profiling method incorporates the uncertainty on the background 
into the measurement by treating the choice of function used to model the background distribution
as a discrete nuisance parameter.
Ordinarily, a nuisance parameter is a continuous variable which affects the measured value 
but is not in itself of any interest.
Nuisance parameters are therefore used to represent systematic uncertainties 
affecting the measurements of the Higgs boson properties.
In the fit used to extract the final results, 
where the value of twice the negative log-likelihood (2NLL) is minimised
several nuisance parameters are ``profiled".
This means their values are allowed to vary, 
either freely or with an additional constraint applied 
which penalises values further from the nominal expectation.
The choice of background function can then be treated in the same way as any other nuisance parameter, 
with the only difference being that its value is discrete rather than continuous.
To construct the final 2NLL curve, 
the so-called ``envelope" of all the possible choices of background function is taken.
This procedure is illustrated in Figure~\ref{fig:sigbkg_envelope}, 
which shows how the 2NLL curve for the fully profiled fit can be approximated 
by taking the envelope of the curves generated with a nuisance parameter fixed to different values.
The discrete profiling method accounts for the uncertainty in the background function analogously;
a different curve is generated by each choice of background function, 
and the final curve represents the envelope of each of these individual curves.
This envelope is necessarily wider than any the curve corresponding to any individual function, 
and therefore returns a greater uncertainty, 
reflecting the unknown shape of the background distribution.
This is illustrated in Figure~\ref{fig:sigbkg_envelope}.

\begin{figure}[hptb]
\centering
\includegraphics[width=\textwidth]{Figures/SigBkg/EnvelopeIllustration.pdf}
\caption{
  An illustration of the discrete profiling, or envelope, method.
  The envelope of all the curves corresponding to fixed values of a given nuisance parameter
  approximates the full curve obtained when the nuisance parameter is profiled.
  Taken from Ref.~\cite{Envelope}.
}
\label{fig:sigbkg_envelope}
\end{figure}

In principle, all functions which provide a sufficiently good fit to the observed \mgg distribution
in data could be included in the discrete profiling method.
To apply the method in practice, 
four different families of functions representing smoothly falling distributions are considered.
These groups, 
for an $N$-parameter function with parameters $p_0, p_1, ..., p_N$ to be determined, are: 
\begin{itemize}
\item Sum of exponential functions, where
\begin{equation*}
f_N(x) = \sum_{i=0}^{N} p_{2i}\exp{\left(p_{2i+1}x\right)}
\end{equation*}
\item Sum of power law functions, where
\begin{equation*}
f_N(x) = \sum_{i=0}^{N} p_{2i}\,x^{-p_{2i+1}}
\end{equation*}
\item Bernstein polynomials, where
\begin{equation*}
f_N(x) = \sum_{i=0}^{N} p_{i}\binom{N}{i}x^{i}\left(1-x\right)^{N-i}
\end{equation*}
\item Laurent series, where
\begin{equation*}
f_N(x) = \sum_{i=0}^{N} p_{i}\,x^{-4 + L(i)},
\end{equation*}
with
\begin{equation*}
L(i) = \sum_{j=0}^{i}(-1)^j j
\end{equation*}
\end{itemize}

Only a subset of functions from each family are considered;
this is necessary to maintain a resonable computational efficiency for the method.
For each family of functions, multiple orders can be included in the final set of candidate functions.
A likelihood fit is performed for each order, 
and then the procedure to decide which orders to include works as follows.
A penalty of equal to the number of parameters in the function is added to the value of the 2NLL;
this prevents functions of arbitrarily high order being chosen.
First, the order is increased until a miminum goodness-of-fit threshold is reached;
there is no need to include functions which do not fit the data well. %threshold is 0.01 from chisq
For each subsequent order, an F-test \cite{Fisher} is performed to gauge the size of the improvement in the fit
quality which the increase in function complexity brings.
This test computes a $p$-value by assuming the difference in 2NLL values 
is distributed as a $\chi^2$, with degrees of freedom equal to 
the difference in number of parameters between the two orders.
If the $p$-value is below a certain threshold, %threshold value 0.05
the higher order function is deemed to constitute a worthwhile improvement, 
is added to the set of functions considered, and the process continues.
Otherwise, the higher order function is not included and the procecedure is complete.
The different functions chosen for the 1J high Tag 0 category 
are shown in Figure~\ref{fig:sigbkg_functions};
it can be seen that different choices of function lead to a different number of events 
when integrating under the signal peak.

\begin{figure}[hptb]
\centering
\includegraphics[width=0.49\textwidth]{Figures/SigBkg/allPdfs_RECO_1J_PTH_120_200_Tag0_2016.pdf}
\includegraphics[width=0.49\textwidth]{Figures/SigBkg/allPdfs_RECO_1J_PTH_120_200_Tag0_2017.pdf}
\caption{
The functions chosen for consideration in the final fit for the 1J high Tag 0 category, 
with the blinded data (black points), 
where the points with $115 < \mgg < \SI{135}{GeV}$ are not shown.
The left plot shows 2016 data, with 2017 data on the right.
}
\label{fig:sigbkg_functions}
\end{figure}

In the final fit, the background function is chosen from the set of candidates as desribed above.
All parameters of each function are free to vary and determined in the fit.
As for the F-test above, 
a regularisation term is added to the 2NLL for each parameter in the chosen function.
This serves to penalise unnecessary complexity, 
and prevents over-fitting of statistical fluctuations observed in data. %not really true - all smoothly falling...
Further details of the discrete profiling method, 
including studies of the bias and coverage,
are contained within Ref.~\cite{Envelope}.

\section{Systematic uncertainties}

In this analysis, the systematic uncertianty associated with the data-driven background estimation 
is handled using the discrete profiling method, as described above.
There are many systematic uncertainties which affect the signal model; 
these are handled in one of two ways.
Uncertainties which modify the shape of the \mgg distribution are incorporated into the signal model
as nuisance parameters, where the mean and width of each Gaussian function can be affected.
These uncertainties are typically experimental uncertainties 
relating to the energy of the individual photons. 
If the shape of the \mgg distribution is unaffected, 
the uncertainty is treated as a log-normal variation in the event yield.
These uncertainties include theory sources
and experimental uncertainties such as those affecting the BDTs used for categorising events.
The magnitude of each uncertainty's impact is determined individually 
for each stage 1 bin in each analysis category.
For log-normal nuisances the impact can be either symmetric or asymmetric,
in the latter case depending on whether the uncertainty is varied in the up or down direction.

\subsection{Theoretical uncertainties}

\subsection{Experimental uncertainties}

\subsection{Correlation of uncertainties}
