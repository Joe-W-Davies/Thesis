\chapter{The High Granularity Calorimeter}
\label{chap:hgcal}

\section{The High Luminosity LHC}

Run~2 of the LHC is now complete, with over $\SI{150}{\fbinv}$ of data collected at $\sqrt{s}\,=\,\SI{13}{TeV}$. %original target was 150 Run 1 plus Run 2
This was achieved partly because the machine was eventually operated at twice its nominal instantaneous luminosity, resulting in values of at $\SI{2e34}{\lumi}$ during 2018.
The second long shutdown (LS2) commenced following the completion of Run~2 and will last until 2021.
During this time various improvements to the LHC will be made, including a substantial upgrade to the injection system.
The machine will also be readied for operation at the increased energy of $\SI{7}{TeV}$ per beam.
However these upgrades will not substantially affect the conditions experienced by the LHC experiments; 
the peak instantaneous luminosity is not envisaged to increase beyond $\SI{2e34}{\lumi}$.
As such no major changes are required to the CMS detector during LS2, although various improvements are planned: 
these include upgrades to the muon system and HCAL barrel.
Therefore the expectation for Run~3, commencing in 2021 with two years of high-availability data-taking in 2022 and 2023, 
is that a further $\SI{150}{\fbinv}$ of data at will be accumulated. 
%See here for details of LS2 and Run 3: https://indico.cern.ch/event/773482/contributions/3213751/attachments/1763994/2863045/LHC-Run2.CMS.Dec18.JW.pdf

Beyond Run~3, the usefulness of running the LHC with its current parameters decreases.
In order to reduce the statistical error on physics measurements by a factor of two, high-availability operation for more than ten years would be required.
Therefore a major upgrade to the LHC is planned, referred to as the Phase~2 upgrade, to maximise its physics reach. 
%insert info about what is actually being upgraded here? %TODO learn a bit about the things below 
%The technological innovations facilitating the upgrade include $\SI{12}{T}$ superconducting magnets, compact and precise superconducting cavities, and new beam collimation technology.
The resulting High~Luminosity~LHC (HL-LHC) \cite{HLLHC} will have an nominal levelled instantaneous luminosity of $\SI{5e34}{\lumi}$, 
permitting a total of $\SI{3000}{\fbinv}$ of data to be collected by the mid-2030s.
The current planned schedule of the future running of the LHC and HL-LHC is summarised in Figure~\ref{fig:hgcal_LHCschedule}.

%FIXME this figure is out of date
\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{Figures/HGCAL/LHCschedule.jpg}
  \caption[Planned LHC and HL-LHC schedule]
  {The planned schedule for the operation of the LHC and its high-luminosity upgrade.}
  %(Need to find a more up-to-date version of this figure).
  \label{fig:hgcal_LHCschedule}
\end{figure}

The corresponding mean pileup is per bunch crossing is 140; however an additional 50\% beyond the nominal value is allowed for in the HL-LHC design, 
which would result in mean pileup values of up to 200.
This constitutes a major change, and the environment will be significantly harsher than with the current LHC conditions, 
posing serious challenges to the detectors in terms of radiation tolerance and reconstruction in high pileup.
Therefore in order to maintain or improve upon the excellent performance exhibited in Run~2, a suite of upgrades to the CMS detector are planned.
The key aspects of the CMS Phase~2 upgrade can be summarised as \cite{UpgradeTP,MTD}:
\begin{itemize}
  \item{\textbf{Tracker:}
  the tracker will suffer significant radiation damage and must be entirely replaced for Phase~2.
  The upgraded tracker will have an granularity, %factor of 4
  increased coverage in the forward region, %up to eta=4
  and be much lighter, resulting in a reduced material budget.
  Furthermore, the design will allow track information to be included in the L1 trigger decision.}
  %enabling powerful background rejection at the earliest event selection step
  \item{\textbf{Endcap calorimeters:}
  the calorimeter endcaps (both ECAL and HCAL) will also be radiation-damaged by the end of Run~3, and will therefore be replaced.
  The replacement design, known as the high granularity calorimeter (HGCAL), will have both electromagnetic and hadronic sections.
  Fine segmentation in each of the longitudnal and transverse directions will facilitate precise measurements of showers in three-dimensions.}
  \item{\textbf{Timing:}
  precision timing of objects will be with the HGCAL and an upgraded ECAL with improved readout eletronics.
  In addition, a dedicated timing layer providing precise timing information for minimum ionising particles will be added.
  By enabling the reconstruction of vertex times, this will greatly improve the capability for pileup rejection.}
  \item{\textbf{Muon endcaps:}
  the CSC forward muon system will be enhanced with additional stations.
  This will increase the forward coverage and maintain muon acceptance at the L1 trigger.}
  %because new bits provide redundancy (only 1.5 to 2.4 doesn't have redundancy in muon system atm) and timing resolution.
  \item{\textbf{Trigger and data acquisition:}
  due to the tracker upgrade, the latency of the L1 trigger at Phase~2 is increased to \SI{12.5}{\micro\second}.
  Combined with upgrades to the front-end electronics of various subdetectors, %barrel calorimeter; the CSCs of the inner rings, and the DT readout.
  this enables an increase of the L1 trigger acceptance rate to \SI{500}{\kilo\hertz}.
  Consequently the data acquisition system must also be upgraded to handle the increase in event rate, event size, and the complexity of high PU reconstruction.}
\end{itemize}
The remainder of this chapter is dedicated to describing the HGCAL in further detail.

\section{Requirements}

The primary challenge which drives the design of the HGCAL is the need to sustain physics performance in the extremely high pileup conditions foreseen at the HL-LHC.
Simulations indicate that the HGCAL will be required to withstand up to 2MGy of total radiation dose, together with a maximum fluence of around $\SI{10e16}{\textrm{n}_{\textrm{eq}}/\textrm{cm}^2}.$
Studies performed in recent years have shown that silicon sensors and the associated electronics retain acceptable performance after exposure to this level of radiation, 
and have hence been chosen as the most reliable active material for the majority of the calorimeter.
The remaining parts of the detector in lower-radiation regions will instead use cheaper plastic scintillator tiles with silicon photomultipliers (SiPMs).

To maintain performance throughout the operation of the HL-LHC it is necessary to inter-calibrate cells to the level of a few percent.
This can be achieved with a sufficiently high signal-to-noise ratio (S/N) for minimum-ionising particles (MIPs), after $\SI{3000}{\fbinv}$.
Consequently small silicon cells with low capacitance are required, resulting in high lateral granularity.
Fine lateral granularity also has many benefits for physics performance, including the ability to separate nearby showers, 
identify narrow jets such as those originating from vector boson fusion (VBF), and minimise the amount of pileup entering energy measurements.
To take advantage of this fine segmentation the calorimeter is required to be dense, thereby preserving the compactness of showers in the transverse direction.
Similarly, fine longitudinal granularity facilitates precise energy measurements, as well as enabling discrimination between different types of shower using the depth profile.
These features are particularly important within the CMS particle flow reconstruction paradigm \cite{ParticleFlow}.
A detector producing three-dimensional images of showers would provide powerful separation of electromagnetic, charged hadronic and neutral hadronic components,
facilitating more precise energy measurements, particularly of jets.

In addition, the HGCAL should be able to perform precise timing measurements enabling excellent pileup rejection, and provide an input to the L1T decision.
The proposed design specification that captures all of these desired features is described in the following section.

\section{Design}

An overview of the HGCAL design is presented in Figure \ref{fig:hgcal_TheHGCAL}.
The HGCAL is composed of an electromagnetic and a hadronic section, called the CE-E and CE-H respectively, covering the pseudorapidity range $1.5\,<\,|\eta|\,<\,3.0$.
The CE-E comprises 28 layers with hexagonal silicon sensors as the active element.
The total depth, including the neutron moderator layer at the front, is \SI{34}{cm}, which corresponds to approximately \SI{26}{$X_0$} and \SI{1.7}{$\lambda$}.
Three different thicknesses of silicon sensors are used, with thickness decreasing as a function of fluence.
Absorbers are made of copper-tungsten alloy and copper plates are used for cooling.
All layers of the CE-E are used for energy measurements, but alternate layers give inputs to the L1 trigger primitive formation.

The CE-H is formed of 12 layers with \SI{35}{mm} thick stainless steel absorber and another 12 where the absorber thickness is \SI{68}{mm}, contributing an additional \SI{9}{$\lambda$} in depth.
The active medium in the CE-H varies as a function of depth and radius, and is determined by the radiation level.
In regions of sufficiently low fluence (those which are nearest the back of the detector and furthest from the beam-pipe), plastic scintillator tiles are used with SiPM readout.
The exact threshold between the scintillator and silicon is determined by the S/N required to measure the MIP response, which is decreased by exposure to radiation.
Further detail on the design specifications of the HGCAL can be found in Ref.~\cite{HGCAL}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{Figures/HGCAL/TheHGCAL.png}
  \caption{A schematic of the HGCAL. Adapted from \cite{HGCAL}.}
  \label{fig:hgcal_TheHGCAL}
\end{figure}

\section{Reconstruction}

\subsection{Electromagnetic objects}

The HGCAL's intrinsic performance measuring the energy of electromagnetic showers is modelled using a dedicated simulation with pileup corresponding to an average of 200 interactions per bunch crossing. 
Energy deposits in a radius of \SI{26}{mm} around a single unconverted photon are summed to estimate the energy resolution, which is shown in Figure \ref{fig:hgcal_PhotonReso}.
The left plot shows that the resolution is approximately constant as a function of $\eta$, and robust against pileup, with only a very small degradation in resolution between PU 0 and PU 200.
This intrinsic performance is further demonstrated by using this method to reconstruct unconverted photon pairs from simulated \Hgg decays, 
where both photons are contained within the fiducial region of the HGCAL and the vertex location is assumed to be known exactly. 
The resulting diphoton mass distribution is shown in Figure \ref{fig:hgcal_PhotonReso}, with resolution of around \SI{1.8}{GeV}.
This value is comparable to the expected resolution of the upgraded CMS barrel calorimeter, representing a substantial improvement relative to Run 2. 

\begin{figure}[h!]
  \centering
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{Figures/HGCAL/SinglePhotonReso.png}
    \caption{The intrinsic energy resolution for single photons in PU 200. 
    The energy is estimated by summing all deposits within a \SI{26}{mm} radius of the generated particle axis. Taken from Ref.~\cite{HGCAL}.}
    \label{fig:hgcal_PhotonReso}
  \end{subfigure}
  \begin{subfigure}{0.46\textwidth}
    \includegraphics[width=\textwidth]{Figures/HGCAL/HggReso.png}
    \caption{The intrinsic diphoton mass resolution of the HGCAL in simulated \Hgg events where both photons are within the fiducial region of the HGCAL. Taken from Ref.~\cite{HGCAL}.}
    \label{fig:hgcal_DiphotonReso}
  \end{subfigure}
\end{figure}

The HGCAL provides more detailed shower information than existing CMS detectors, 
and it is envisaged that eventually a sophisticated four-dimensional particle flow approach will be used to incorporate as much of this information as possible. 
In the meantime, more straightforward approaches to reconstruction have been developed, 
in order to understand which approaches are feasible and to produce object and physics-level results that demonstrate the potential of the detector. 

%Furthermore, no use is yet made of the intrinsic timing capabilities of the silicon sensors, which will be invaluable in reducing the amount of out-of-time pileup at the HL-LHC. 
The current method begins by clustering hits in each two-dimensional (2D) layer independently, using an imaging algorithm \cite{ClusteringAlgo}.
These 2D or layer clusters are then associated together in depth to form so-called multiclusters. 
This step is performed by starting with the highest energy 2D cluster, then adding any other clusters sufficiently close in $\eta-\phi$ space. 
TODO add detail here on how the algorithms proceed, why they works, and the optimisation process.
TODO consider figures??
TODO and then also mention how it is used in all TDR results.
%It is anticipated that this two-step process could be improved by performing 3D clustering directly, but this remains to be studied in detail. 

Electromagnetic objects are then formed using a superclustering procedure very similar to the one utilised in Run 2, 
collecting together showers which have been spread out in the $\phi$ by the magnetic field. 
To perform this step, multiclusters are used as inputs to the existing Run~2 algorithm; no re-optimisation is performed.
TODO add stuff about the results, mass resolution obtained etc.

%Electrons defined in this way are used to test the ability of the HGCAL to discriminate between signal and background processes. 
%Lateral and longitudinal shower shape variables, along with tracking information, are used as inputs to a Boosted Decision Tree (BDT) classifier. 
%For a 95\% signal efficiency, the background efficiency is 1\% for electrons with $\pt\,>\,\SI{20}{GeV}$ , comparable to the Run 2 value. 
%An improvement in performance is seen when both lateral and longitudinal shape variables are added to a classifier using track-only information. 
%Additional studies applying the anti-$k_T$ algorithm directly on calorimeter hits to form jets have also been performed.
%Constructing a pileup jet identification using just two variables, one each related to the lateral and longitudinal jet shape, 
%was shown to be permit an acceptable L1 trigger rate of 10kHz for a signal efficiency of 80\%. 

\subsection{Hadronic objects}

Single hadrons were reconstructed using a so-called megaclustering procedure, where layer clusters within a truncated cone are combined to form the object. 
TODO add details on this, how the megacluster is formed, why it works.
For these more dispersed hadronic showers, the resolution substantially improves once the contribution of pileup is subtracted.
The PU subtraction was implemented by removing the total energy of a similar cone randomly rotated in $\phi$. 
The energy resolution for a single pion with $\pt\,=\,\SI{25}{GeV}$ before and after the subtraction is shown in Figure \ref{fig:hgcal_SingleMegacluster}. 
The megaclustering algorithm is shown to yield adequate energy resolution of around 20\%. 
It is also robust against pileup; Figure~\ref{fig:hgcal_MegaclusterVsPt} shows the modest worsening between PU 0 and PU 200 that decreases quickly as a function of \pt.
TODO be clear that this is not used in the following study/general TDR results (sim cluster stuff instead).

\begin{figure}[h!]
  \centering
  \begin{subfigure}{0.44\textwidth}
    \includegraphics[width=\textwidth]{Figures/HGCAL/SingleMegacluster.png}
    \caption{The distribution of the energy resolution for single $\pt\,=\,\SI{25}{GeV}$ pions reconstructed using the megaclustering algorithm, 
    before and after pileup subtraction. Taken from Ref.~\cite{HGCAL}.}
    \label{fig:hgcal_SingleMegacluster}
  \end{subfigure}
  \begin{subfigure}{0.54\textwidth}
    \includegraphics[width=\textwidth]{Figures/HGCAL/MegaclusterVsPt.png}
    \caption{The mean energy resolution for single $\pt\,=\,\SI{25}{GeV}$ pions reconstructed using the megaclustering algorithm,after pileup subtraction, as a function of \pt. Taken from Ref.~\cite{HGCAL}.}
    \label{fig:hgcal_MegaclusterVsPt}
  \end{subfigure}
\end{figure}

\subsection{Future development}
TODO stuff about 3D clustering and neural nets here.
TODO also discussion of the impact of timing information.
TODO and impact of tracking and full particle flow

\section{Physics performance}

One of the main benefits of the HGCAL upgrade will be the ability to separate quark-initiated jets from those originating from gluon emission.
This is illustrated in a study of the ggH and VBF production modes in the \Hgg decay channel, using simulated events under HL-LHC conditions.
%Jets are reconstucted using the Pileup Per Particle Identification (PUPPI) technique \cite{PUPPI}, which is also used in some Run~2 analyses.

Jets intiated by gluons tend to be softer and more dispersed than quark-initiated jets, which are more highly collimated and contain fewer particles. 
Therefore variables relating to the jet shapes can be used to discriminate between the two, and in turn between the ggH and VBF production processes.

Three jet shape variables are used to construct a BDT referred to as the jet shape BDT.
A second BDT, known as the dijet BDT, uses additional kinematic variables including those of the photons; 
its inputs are identical to the dijet BDT used in the Run~2 analysis, but with the three jet shape variables added.
The same training procedure as that used in the Run~2 analysis was followed, 
where the VBF events are treated as signal, with the ggH as background.
The performance of the two classifiers are illustrated below in Figure \ref{fig:hgcal_VBFvsGGH}.
The area under the ROC curve is 0.71 for the jet shape BDT, and 0.79 for the dijet BDT.
For comparison, the area under the ROC curve of the Run 2 dijet BDT was 0.75.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.65\textwidth]{Figures/HGCAL/VBFvsGGH.png}
  \caption{The selection efficiency for VBF and ggH events for two different BDT setups. Taken from Ref.~\cite{HGCAL}.}
  \label{fig:hgcal_VBFvsGGH}
\end{figure}

Separately, another BDT is trained to reduce the amount of background entering analysis categories.
The VBF events are treated as signal, trained against prompt-prompt, prompt-fake and fake-fake \Hgg backgrounds.
This classifier uses only the kinematics of the the two photons as inputs. 
It is less powerful than the Run~2 equivalent because not all the Run~2 inputs are reproduced, 
including the photon identification score, the mass resolution estimates, and the vertex proability estimate.

A 2D scan was used to choose cut values on both the dijet and background BDTs, and generate the working points in Table \ref{tab:hgcal_yields}.
The number of signal events selected, the fraction of ggH and VBF events, and the background per GeV are shown.
Also included is a Run~2 working point, consisting of all the events entering the analysis' three VBF tags, for comparison.

\begin{table}
  \centering
  \begin{tabular}{ r | c |  c |  c |  c }
  \multirow{2}{*}{Event Categories} &\multicolumn{3}{c|}{SM 125 GeV Higgs boson expected signal} & Bkg \\
    &  Total & ggH & VBF & per GeV \\
  \hline
  WP 0 &    750   &  25.4 \%  &  74.6 \%  &  678  \\
  WP 1 &    1275  &  35.9 \%  &  64.1 \%  &  876  \\
  WP 2 &    1926  &  45.8 \%  &  53.2 \%  &  1353 \\
  Run 2 WP &    3878 &  42.0 \%  &  58.0 \%  &  1984 \\
  \end{tabular}
  \caption{The signal and background yields for three working points.
  The dijet BDT cut is varied, with a fixed cut on the background BDT.
  Number of events given is for \SI{3000}{\fbinv} of collected data. 
  The Run~2 WP contains the sum of selected events in all three VBF categories, extrapolated to \SI{3000}{\fbinv}.}
  \label{tab:hgcal_yields}
\end{table}

%With \SI{3000}{\fbinv} of data, a suitably chosen cut on this BDT would yield 750 signal events, with 75\% VBF purity, for a background of 678 events per GeV, comparable to the Run~2 values. 
These results show that performance comparable to that in Run~2 is achieved desipite the increase in pileup.
The background numbers are slightly higher than in Run~2, but this is expected to be reduced by a classifier using further photon quality variables.

\section{Beam tests}

To validate the design of the HGCAL and ensure its behaviour is well-modelled by simulation, beam tests have been conducted at both CERN and Fermilab sites in 2016 and 2017. 
Prototype silicon modules representative of those in both the CE-E and CE-H were built, with plastic scintillator tiles modified from an existing detector developed by the CALICE collaboration.
Comparisons of the measured electron energy resolution and hadronic shower shape to simulation show good agreement.
These prototypes were not the full design thickness, so the performance is not representative of the final detector.
However the results constitute the first demonstration that the HGCAL behaves as predicted by simulation; observed distributions match those predicted by simulation to within 5\%. 
Furthermore, the tests confirm the intrinsic timing capabilities of the silicon sensors, with timing resolution measured to be less than \SI{30}{\pico\second}.
The timing performance of the silicon was also confirmed to be a function of S/N only, and not degrade with increasing radiation exposure. 

The HGCAL project is designed to replace the existing CMS endcap calorimeters and withstand the harsh HL-LHC conditions for its full lifetime.
Offering unprecedented granularity in lateral and longitudinal directions, it is expected to provide detailed three-dimensional images of showers in addition to precision timing information from the silicon sensors. 
This will allow for powerful pileup rejection, sophisticated pattern recognition within CMS particle flow, and precise energy measurements. 
The recent technical design report showcased the potential of the detector, and between now and installation in 2026 much work remains to be done to overcome the many technical challenges ahead.
