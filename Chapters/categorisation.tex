\chapter{Event Categorisation}
\label{chap:categorisation}

\section{Introduction}

The event selection consists of the preselection described in Chapter~\ref{chap:objects}, 
and in addition requires the two leading preselected photon candidates to have 
$\pt^{\gamma 1} > \mgg/3$ and $\pt^{\gamma 2} > \mgg /4$ respectively, 
with an invariant mass in the range $100 < \mgg < \SI{180}{GeV}$.
Both photons must also satisfy the
pseudo-rapidity requirement $|\eta|<2.5$ and must not be in the barrel-endcap
transition region $1.44 < |\eta| < 1.57$.
The above $\eta$ requirement is applied to the photo supercluster
position, and the requirement on the photon $\pt$ is applied 
after the vertex assignment.

The \Hgg analysis depends on the ability to distinguish the narrow signal peak 
from the smoothly falling background in the diphoton mass distribution.
Selected events are therefore subject to further categorisation, in order to 
increase the ratio of the number of signal events to the number of background events (S/B).
This enhances the sensitivity of the analysis, 
reducing the expected uncertainties on the measured quantities.

Analysis categories are also constructed to target events in which the Higgs boson was 
produced by a specific production mechanism. 
This is achieved using the information provided by additional objects in the event, 
alongside the two photons arising from the Higgs boson decay.
As well as facilitating measurements of cross sections corresponding 
to individual production mechanisms, these dedicated categories also enable the S/B to be improved.

In the previous \Hgg analysis using the 2016 dataset \cite{HIG-16-040}, 
dedicated categories targeting the VBF, ttH, and VH modes were construced, 
with the remaining so-called ``Untagged" categories composed mostly of ggH events.
Here a similar approach is employed, 
but with additional divisions targeting individual stage 1 bins for the ggH and VBF processes.
Events selected by the 2017 \Hgg analysis targeting ttH production, 
described in Ref.~\cite{HIG-18-018}, are not included in this analysis. 
This is achieved by first applying the same selection criteria used to construct the ttH categories, 
and then removing them from further consideration. 
There are no dedicated analysis categories for VH production.

The categorisation targeting ggH is based on the reconstructed diphoton transverse momentum (\ptgg) 
and the number of jets in the event. 
A BDT referred to as the diphoton BDT is then used to reduce the amount of background. 
The VBF analysis categories make use of the same diphoton BDT 
to reduce the number of background events. 
Additionally, a BDT targeting the kinematics of the characteristic VBF dijet system, 
known as the dijet BDT, is utilised to reduce the contamination from ggH events.

Due to the conditions differing between the two years, 
the analysis is optimised separately for the 2016 and 2017 datasets. 
A simultaneous fit to the categories from both years is then performed to estimate the 
values of the parameters of interest and their uncertainties (described in Chapter ~\ref{chap:results}).
The remainder of this chapter describes in detail the training of the diphoton and dijet BDTs, 
and their use in the cateogory optimisation process for both ggH and VBF events

\section{Gluon fusion categorisation}
\subsection{Signal bin definitions}

At stage 1 of the STXS framework, 
the gluon fusion process (ggH) is divided into a total of eleven particle level bins.
The events are split first by the number of jets, 
defined at particle level using the anti-$k_T$ algorithm with radius parameter 0.4 
and jet $\pt > \SI{30}{GeV}$~\cite{AntiKt}.
All stable particles except for the decay products of the Higgs boson 
are included in the anti-$k_T$ clustering.
There are zero (0J), one (1J), and greater than or equal to two (2J) jets bins.
In events with at least one jet, a further splitting 
by the value of the transverse momentum of the Higgs boson (\ptH) is performed. 
Four bins are defined by boundaries placed at 60, 120, and \SI{200}{GeV}.
These bins are denoted as low, medium (med), high, and BSM, respectively.
Finally, a separate ggH region is dedicated to the vector boson fusion-like phase space, 
for which a pair of jets (a dijet) with invariant mass $m_{jj} > \SI{400}{GeV}$ 
and difference in pseudorapidity $\Delta\eta > 2.8$ is required.
The dijet is formed from the two leading jets in the event.
This region is split into two-jet-like ($\ptHjj < \SI{25}{GeV}$) 
and three-jet-like ($\ptHjj > \SI{25}{GeV}$) bins, 
where \ptHjj is defined as the transverse momentum of the Higgs boson plus dijet system.
Each bin is exclusive; events included in the VBF-like region are not included in the other ggH 2J bins.
A table summarising the definition of each bin, its cross section, and the fraction of the 
total ggH cross section is shown in Table \ref{tab:ggHfractions}.
The inclusive ggH cross section is $48.52~\textrm{pb}$ at $\mH = \SI{125.09}{GeV}$, 
computed to an accuracy of three loops in perturbative quantum chromodynamics (QCD) 
and next-to-leading order (NLO) in electroweak perturbations (EW) 
\cite{YR4,Anastasiou2015,Anastasiou2016}.
Of this approximately $44.2~\textrm{pb}$ is within $|y_H| < 2.5$.

\begin{table}
  \begin{centering}
    \begin{tabular}{ r | c | c | c } 
    \hline
    Region & Definition & Fraction & Cross section (pb) \\ 
    \hline
    0J          & Exactly zero jets, any $\ptH$               & 60.0\% & 26.49 \\ 
    \hline
    1J low      & Exactly one jet, $\ptH <$ 60 GeV            & 15.4\% & 6.79  \\
    \hline
    1J med      & Exactly one jet, 60 GeV $< \ptH <$ 120 GeV  & 10.4\% & 4.61  \\ 
    \hline
    1J high     & Exactly one jet, 120 GeV $< \ptH <$ 200 GeV & 1.7\% & 0.76   \\
    \hline
    1J BSM      & Exactly one jet, $\ptH >$ 200 GeV           & 0.4\% & 0.16   \\ 
    \hline
    2J low      & $\ge$ two jets, $\ptH <$ 60 GeV             & 2.9\% & 1.26   \\
    \hline
    2J med      & $\ge$ two jets, 60 GeV $< \ptH <$ 120 GeV   & 4.5\% & 2.00   \\ 
    \hline
    2J high     & $\ge$ two jets, 120 GeV $< \ptH <$ 200 GeV  & 2.3\% & 1.00   \\
    \hline
    2J BSM      & $\ge$ two jets, $\ptH >$ 200 GeV            & 1.0\% & 0.43   \\ 
    \hline
    \multirow{2}{*}{VBF-like 2J} & $\ge$ two jets, $\ptH < 200$ GeV, $|\Delta\eta| > 2.8$, & \multirow{2}{*}{0.6\%} & \multirow{2}{*}{0.27} \\ 
                                 & $m_{jj} >$ 400 GeV, $\ptHjj <$ 25 GeV &                        &                       \\ 
    \hline
    \multirow{2}{*}{VBF-like 3J} & $\ge$ two jets, $\ptH < 200$ GeV, $|\Delta\eta| > 2.8$, & \multirow{2}{*}{0.9\%} & \multirow{2}{*}{0.38} \\ 
                                 & $m_{jj} >$ 400 GeV, $\ptHjj >$ 25 GeV &                        &                       \\ 
    \hline
    \end{tabular}
    \caption{The particle level definition of each ggH stage 1 bin 
    and the corresponding fractional and absolute cross sections.
    The fractions are estimated from simulated ggH \Hgg events 
    within the region $|y_H| < 2.5$.
    Details of the simulated samples can be found in Chapter~\ref{chap:objects}.
    Each bin is exclusive; 
    events included in the VBF-like region are not included in the other ggH 2J bins.
    }
    \label{tab:ggHfractions}
  \end{centering}
\end{table}

In order to measure the stage 1 bins individually, 
categories must be constructed which differentiate between them.
Thus the reconstruction level event categorisation is designed to target all of the bins 
to which some sensitivity can be obtained in the \Hgg decay channel with this dataset.
In this case, the majority of the ggH stage 1 bins can be measured individually. 
The exception is the two VBF-like bins, 
which are very difficult to separate from true VBF production.
Therefore events not entering the categories targeting VBF production 
can be assigned to one of nine target ggH stage 1 bins.
This assignment is performed using the reconstructed 
diphoton transverse momentum (\ptgg) and the number of jets; 
the detector level equivalents of the particle level quantities used to define the bins.
%More advanced assignment procedures have been studied, 
%and show modest improvement with respect to this version; 
%these are described in Chapter~\ref{chap:}
The diphoton BDT is then used to reduce the amount of background 
and therefore improve the analysis' sensitivity to each ggH stage 1 bin.

\subsection{Diphoton BDT}

The diphoton BDT is trained to discriminate signal events 
(where two photons are produced from the decay of a Higgs boson) from background events 
(where two photons are produced by other SM processes).
The goal of the classifier is to assign high scores to signal-like events;
requirements can then be placed on the output score of the classifier 
to increase the S/B for the analysis categories.
The criteria for an event to be signal-like include having signal-like photon kinematics, 
and having high scores from the photon identification BDT.
In addition, events with a good diphoton mass resolution are preferred 
since this reduces the amount of background present under the signal peak.

To train the diphoton BDT, simulated events from the ggH, VBF, ttH 
and VH production modes are treated as signal. 
%Something about using powheg here.
Simulated background events include the contributions from prompt-prompt, prompt-fake, 
and fake-fake sources.
For both signal and background, 
events are weighted in accordance with the SM process cross-section.
Furthermore, events from the QCD sample are down-weighted by a factor of 25;
the limited number of events in this sample results in very high per-event weights, 
which causes the classifier to over-estimate the importance of individual events.
All events are required to satisfy the analysis selection criteria.
The input variables to the classifier are the kinematic properties of the diphoton system, 
a per-event estimate of the diphoton mass resolution, 
and the photon identification scores of each photon.
The set of input variables is chosen such that the value of the Higgs boson mass cannot be inferred;
it is for this reason that the photon momenta are divided by the diphoton mass.
%TODO add a sentence later saying that it's checked no mass bias is introduced.
The full list of input variables is as follows:
\begin{itemize}
\item the transverse momentum of each photon, divided by the diphoton mass, $\pt^{1,2}/\mgg$;
\item the pseudorapidity of each photon $\eta^{1,2}$;
\item the cosine of the angle between the two photons in the transverse plane, $\cos{\Delta\phi}$;
\item the output score of the photon identification BDT for each photon;
\item the per-event relative mass resolution estimate, 
      under the hypothesis that the mass was computed using the correct primary vertex \srv/\mgg;
\item the per-event relative mass resolution estimate, 
      under the hypothesis that the mass was computed using an incorrect primary vertex \swv/\mgg;
\item the per-event probability estimate that the correct primary vertex was selected, $p_{vtx}$,
      which is the output of the vertex probabilty BDT described in Chapter~\ref{chap:objects};
\end{itemize}

The per-event relative mass resolution under the correct vertex hypothesis 
depends only on the photon energy measurements performed by the ECAL.
It is calculated by propagating the photon energy resolution estimates, 
assuming the resolutions are independent and Gaussian distributed:

\begin{equation}
  \srv/\mgg = \frac{1}{2} \sqrt{\left(\frac{\sigma_{E_1}}{E_1}\right)^{2}
                              + \left(\frac{\sigma_{E_2}}{E_2}\right)^{2}}
\end{equation}

where both the energy ($E_{1,2}$) and energy resolution ($\sigma_{E_{1,2}}$) estimates 
are obtained from the regression described in Chapter~\ref{chap:objects}.
Under the incorrect vertex hypothesis, an additional term must be added to account for the 
worsening in the diphoton mass resolution due to the uncertainty of the vertex position.
The distance between the correct and incorrect vertices is assumed to follow a Gaussian distribution
with a width dependent on the size of the beamspot.
The contribution from the vertex ($\sigma_{vtx}$) can then be computed analytically 
using the measured positions of the photons in the detector 
and combined in quadrature with \srv to calculate \swv:
%TODO understand this better

\begin{equation}
  \swv/\mgg = \sqrt{\left(\frac{\srv}{\mgg}\right)^{2}
                              + \left(\frac{\sigma_{vtx}}{\mgg}\right)^{2}}
\end{equation}

An example of the distribution for one of the input kinematic variables, the lead photon $\pt/\mgg$, 
is shown in Figure~\ref{fig:cat_ptominput}.
The background, split into its prompt--prompt, prompt--fake, and fake--fake components, 
is compared to the data sidebands (region with 115 < \mgg < 135 GeV excluded).
Good agreement between the two is observed.

%TODO put plots in https://escott.web.cern.ch/escott/FinalFits/Stage1STXS/Pass2/2016/inputPlots/
\begin{figure}[hptb]
\centering
\includegraphics[width=0.49\textwidth]{Figures/Categorisation/leadptom_2016.pdf}
\includegraphics[width=0.49\textwidth]{Figures/Categorisation/leadptom_2017.pdf}
\caption{
  Distribution of the lead photon $\pt/\mgg$ in background (stacked histogram) 
  and data (black points) events.
  The statistical uncertainty is shown by the orange band.
  The left plot shows 2016 data and MC,
  with 2017 data and MC on the right.
}
\label{fig:cat_ptominput}
\end{figure}

None of the input variables above encode the preference for events with good mass resolution.
For this purpose, an additional weight is applied to signal events which increases the relative 
importance of events with better values of the diphoton mass resolution. 
The weight applied, $w_{res}$, is given by the formula

\begin{equation}
  w_{res} = \frac{p_{vtx}}{\srv} + \frac{1-p_{vtx}}{\swv}
\end{equation}

This ensures that the classifier output score is higher for events 
which have a relatively low expected diphoton mass resolution.
Finally, the signal and background samples are divided into a training and a test set,
cotaining 70\% and 30\% of the total number of events respectively.
With this configuration of input variables, signal and background events, and event weights, 
the classifier is trained and its performance evaluated using the XGBoost package~\cite{XGBoost}.

The performance of the training is evaluated using a receiver operating chararteristic (ROC) curve, 
where the signal efficiency is plotted as a function of the background efficiency,
with each point corresponding to a specific threshold value placed on the classifier output score.
The area under the ROC curve is used to gauge the performance of a given classifier; 
a higher area corresponds to more effective discrimination between signal and background.

%TODO add some ROC curve and feature importance plots here.

This metric is utilised to compare the performance of the classifier training 
with different values of the BDT's so-called hyper-parameters.
These hyper-parameters are arbitrary parameters of the BDT, 
which are not learned but instead affect how the learning algorithm behaves.
The most important hyper-parameters affect the extent to which 
the algorithm learns the specific detail of the training sample provided.
If a classifier has learnt features from the training set inputs
which are due solely to statistical fluctation and not a more general feature of the signal, 
over-training can result.
To check for potential over-training, the training score as measured by area under the ROC curve 
of the trainign set is compared with the score from the independent test set.
If the test set score is significantly higher than that of the training set, 
the classifier has been over-trained.
The default classifier hyper-parameters display no overtraining.
A coarse hyper-parameter optimisation procedure is then performed, 
in which each hyper-parameter is varied individually.
No significant improvement without overtraining is found, 
and therefore the default training parameters are used.

An additional parameter which can be optimised is the relative weight 
of the signal and background samples.
With the default sample weights corresponding to the SM sample cross section, 
the two classes are highly imbalanced.
This imbalance can cause suboptimal learning in the classifier.
To check this, the classifier is trained in a scenario 
where the signal event weights are increased by a uniform factor, 
such that the total sum of weights for signal and background is equal.
This is purely as a technical change to the training 
designed to improve the learning outcomes of the classifier.
When evaluating the performance, the default event weights are used.
Equalsing the total weights in this way results in a modest improvement in performance.
The areas under the ROC curves become 0.80 and 0.82 for the 2016 and 2017 datasets respectively.
The improvement is relatively small, 
but is however larger than the variation 
in training performance induced by either changing the random seed used for training or 
choosing different subsets of the input data for training and testing.
Therefore we choose to use this training scenario for the final classification.
The hyper-parameter optimisation procedure was repeated 
and again no significant improvement was obtained.

%TODO add rationale for validation and Zee plots here

\subsection{Category definitions}

\section{Vector boson fusion categorisation}
\subsection{Signal bin definitions}
\subsection{Dijet BDT}
\subsection{Category definitions}
