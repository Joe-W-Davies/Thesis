\chapter{Event Categorisation}
\label{chap:categorisation}

\section{Introduction}

\sloppy The analysis event selection consists of the preselection described in Chapter~\ref{chap:objects}, 
and in addition requires the two leading preselected photon candidates to have 
${\pt^{\gamma 1} > \mgg/3}$ and~${\pt^{\gamma 2} > \mgg /4}$ respectively, 
with an invariant mass in the range $100 < \mgg <~\SI{180}{GeV}$. 
The requirements on the scaled photon transverse momenta prevent distortions 
at the lower end of the mass spectrum.
Both photons must also satisfy the
pseudo-rapidity requirement $|\eta|\,<\,2.5$ and must not be in the barrel-endcap
transition region $1.44\,<\,|\eta|\,<\,1.57$;
the reduced containment in this transition region worsens the photon energy resolution.
The above $\eta$ requirement is applied to the photon supercluster
position, and the requirement on the photon $\pt$ is applied 
after the vertex assignment.

The \Hgg analysis depends on the ability to distinguish the narrow signal peak 
from the smoothly falling background in the diphoton mass distribution. 
Selected events are therefore subject to further categorisation, in order to 
increase the ratio of the number of signal events to the number of background events (S/B).
This enhances the sensitivity of the analysis, 
reducing the expected uncertainties on the measured quantities.

Analysis categories are also constructed to target events in which the Higgs boson was 
produced by a specific production mechanism. 
This is achieved using the information provided by additional objects in the event, 
alongside the two photons arising from the Higgs boson decay.
As well as facilitating measurements of cross sections corresponding 
to individual production mechanisms, these dedicated categories also enable the S/B to be improved.

In the previous \Hgg analysis using the 2016 dataset \cite{HIG-16-040}, 
dedicated categories targeting the VBF, ttH, and VH modes were constructed, 
with the remaining so-called ``Untagged" categories composed mostly of ggH events.
Here a similar approach is employed, 
but with additional divisions targeting individual stage 1 bins for the ggH and VBF processes.

The dataset collected by CMS in 2017 has already been used in an analysis targeting ttH production,
which is described in Ref.~\cite{HIG-18-018}.
In order to ensure that the set of data events included in this analysis is orthogonal 
from those included in Ref.~\cite{HIG-18-018}, 
a veto is applied to events selected for categories targeting ttH production.
The criteria used to select ttH include two dedicated BDTs, 
one targeting events where at least one W boson decays leptonically
and the other preferentially selecting fully hadronic W boson decays.
Input variables to the two BDTs include photon, lepton, and jet kinematics, 
information relating to $b$-tagging, and missing transverse momentum.
The veto is implemented by applying these criteria used to construct the ttH categories, 
and then removing them from further consideration. 
Around fifteen events are expected to be removed as a result, 
most of which would otherwise have populated the ggH 2J categories with higher \ptH values.
Furthermore, there are no dedicated analysis categories for VH production; 
the number of events in the diphoton decay channel with this dataset 
is insufficient to measure any of the individual stage 1 bins~\cite{YR4}.

The categorisation targeting ggH is based on the reconstructed diphoton transverse momentum (\ptgg) 
and the number of jets in the event. 
A BDT referred to as the diphoton BDT is then used to reduce the amount of background. 
The VBF analysis categories make use of the same diphoton BDT 
to reduce the number of background events. 
Additionally, a BDT targeting the kinematics of the characteristic VBF dijet system, 
known as the dijet BDT, is utilised to reduce the contamination from ggH events.

Due to conditions differing between the two years, 
the analysis is optimised separately for the 2016 and 2017 datasets. 
A simultaneous fit to the categories from both years is then performed to estimate the 
values of the parameters of interest and their uncertainties (described in Chapter ~\ref{chap:results}).
The following section provides an introduction to BDTs and how they are used in physics analyses.
The remainder of the chapter then describes in detail the training of the diphoton and dijet BDTs, 
and their use in the category optimisation process for both ggH and VBF events.

\newpage

\section{Boosted decision trees}
\label{sec:BDTs}
In the CMS \Hgg analysis, BDTs are used for several purposes.
Generally, the purpose of the BDT is to discriminate between 
one signal-like and one background-like process.
The BDT provides a per-event output score which indicates how signal-like the event is, 
and a criterion can be applied on this output score when selecting or categorising events.
This section gives a brief explanation of what BDTs are and how they are trained.

A BDT is an example of a machine learning algorithm which takes as input a set of features, 
a set of training parameters, and a loss function, 
and from those inputs returns an output score~\cite{ElementsLearning}.
The so-called feature vector $\vec{x}$ is a set of real values for each event
which can be used to discriminate between signal and background processes. 
Examples of features used in the analysis include kinematic variables such as \pt or $\eta$.
In addition, when training the BDT, the target outcome $y$ must be provided.
The events used for training typically come from simulation, where the truth process is known.
For all the cases considered in this analysis, $y$ simply takes the value 1 for signal events
and 0 for background events.
The training parameters $\vec{w}$, also referred to as hyper-parameters, 
are parameters which control the behaviour of the learning procedure.
The loss function $L$ defines the metric on which the learning procedure is optimising.
The output score for events being evaluated is denoted $Y$.

A BDT is an ensemble of decision trees (DTs), 
which are so-called base learners that recursively split the input feature space into distinct regions
by applying binary partitions at each node.
Once a given branch reaches its final node where no further splitting is performed, 
an output value is assigned to that region.
The training procedure for an individual DT involves considering many possible configurations
for the tree and computing the loss function for each one.
The point at which the training process terminates depends upon the hyperparameter values.
For example, there may be a restriction on the tree depth, 
meaning the number of binary partitions permitted for each branch.
In this way an optimal configuration is chosen, 
and any given input for evaluation (without the target label $y$) 
will be placed in a single region and assigned the corresponding score.

The boosting procedure combines individual DTs into a more powerful learner.
An iterative training process enables the existing models to be improved upon; 
each new learner corrects the previous version.
The final BDT output function then consists of a weighted sum of individual DTs.
A type of boosting method known as gradient boosting trains each successive tree 
on the residuals, or errors, of the existing function, 
thereby attempting to correct the mistakes made by existing trees.
This is equivalent to subtracting the derivative of a squared error loss function;
this is the reason the method is given the name gradient boosting.
This gradient boosting procedure can be generalised to use pseudo-residuals, 
which are given by the derivative of an arbitrary differentiable loss function.
The loss function chosen here is distinct from that used to train the individual DTs.
Once a new tree is trained on these pseudo-residuals, it is then added to the existing function, 
itself a weighted sum of trees.
The weight of the new tree is assigned by minimising the loss of the new summed function.
This procedure is repeated until a given performance threshold or other stopping criterion is met.

The performance of a given BDT training is evaluated on a sample independent 
of that used to train it, in order to ensure that 
the features learned by the BDT generalise beyond the specific training dataset used.
If the BDT's performance is greater on the training dataset than the so-called test dataset, 
it is said to have overfitted, or been overtrained.
This means it has placed too much importance on statistical fluctuations on the training dataset
which will not be present in other samples.
The hyperparameters chosen for a given training will affect whether or not the BDT is overtrained.
For example, the learning rate reduces the weight assigned to each tree in the iterative learning
process, which is analogous to the step size in gradient descent methods.
A smaller learning rate will reduce the chance of overfitting 
but also increase the time taken for the training to converge.
For this reason, an optimisation procedure is normally used to choose the hyperparameters
in such a way that maximises performance without overtraining the BDT.

\section{Gluon fusion categorisation}
\subsection{Signal bin definitions}

At stage 1 of the STXS framework, 
the gluon fusion process (ggH) is divided into a total of eleven particle level bins.
The events are split first by the number of jets, 
defined at particle level using the anti-$k_T$ algorithm with radius parameter 0.4 
and jet $\pt > \SI{30}{GeV}$~\cite{AntiKt}.
All stable particles except for the decay products of the Higgs boson 
are included in the anti-$k_T$ clustering.
There are zero (0J), one (1J), and greater than or equal to two (2J) jets bins.
In events with at least one jet, a further splitting 
by the value of the transverse momentum of the Higgs boson (\ptH) is performed. 
Four bins are defined by boundaries placed at 60, 120, and \SI{200}{GeV}.
These bins are denoted as low, medium (med), high, and BSM, respectively.
Finally, a separate ggH region is dedicated to the vector boson fusion-like phase space, 
for which a pair of jets (a dijet) with invariant mass $m_{jj} > \SI{400}{GeV}$ 
and difference in pseudorapidity $\Delta\eta > 2.8$ is required.
The dijet is formed from the two leading jets in the event.
This region is split into two-jet-like ($\ptHjj < \SI{25}{GeV}$) 
and three-jet-like ($\ptHjj > \SI{25}{GeV}$) bins, 
where \ptHjj is defined as the transverse momentum of the Higgs boson plus dijet system.
Each bin is exclusive; events passing the the VBF-like selection enter the VBF-like region
and are not considered for the other ggH 2J bins.
A table summarising the definition of each bin, its cross section, and the fraction of the 
total ggH cross section is shown in Table \ref{tab:cat_ggHfractions}.
The inclusive ggH cross section is $48.52~\textrm{pb}$ at $\mH = \SI{125.09}{GeV}$, 
computed to an accuracy of three loops in perturbative QCD 
and next-to-leading order (NLO) in EW perturbations~\cite{YR4,Anastasiou2015,Anastasiou2016}.
Approximately $44.2~\textrm{pb}$ of this is within $|y_H| < 2.5$.

\begin{table}
  \begin{centering}
    \input{Tables/ggHbins.tex}
    \caption[Particle level definitions of the ggH stage 1 STXS bins.]
    {
      The particle level definition of each ggH stage 1 bin 
      and the corresponding fractional and absolute cross sections.
      The fractions are estimated from simulated ggH \Hgg events 
      within the region $|y_H| < 2.5$.
      Details of the simulated samples can be found in Chapter~\ref{chap:objects}.
      Each bin is exclusive; events passing the the VBF-like selection enter the VBF-like region
      and are not considered for the other ggH 2J bins.
    }
    \label{tab:cat_ggHfractions}
  \end{centering}
\end{table}

\subsection{Categorisation strategy}

In order to measure the stage 1 bins individually, 
categories must be constructed which differentiate between them.
Thus the reconstruction level event categorisation is designed to target all of the bins 
to which some sensitivity can be obtained in the \Hgg decay channel with this dataset.
In this case, the majority of the ggH stage 1 bins can be measured individually. 
The exceptions are the two VBF-like bins, 
which are difficult to separate from true VBF production.
Therefore events not entering the categories targeting VBF production 
can be assigned to one of nine target ggH stage 1 bins.
This assignment is performed using the reconstructed 
diphoton transverse momentum (\ptgg) and the number of jets -- 
which are respectively the detector level equivalents 
of the particle level quantities \ptH and number of jets used to define the bins.
The same boundaries are used as for the particle level bins; 
0, 60, 120 and \SI{200}{GeV} in \ptgg for 1J and 2J events, 
and inclusive in \ptgg for 0J events.
%More advanced assignment procedures have been studied, 
%and show modest improvement with respect to this version; 
%these are described in Chapter~\ref{chap:conclusion}
Once a target signal bin has been assigned, 
the diphoton BDT is used to further divide the events into up to three categories.
Requirements are placed on the output score of the diphoton BDT, 
with the category with the highest threshold referred to as ``Tag 0", 
the next highest as ``Tag 1", and so on.
This reduces the amount of background 
and therefore improves the analysis' sensitivity to each ggH stage 1 bin.

\subsection{Diphoton BDT}

The diphoton BDT is trained to discriminate signal events 
(where two photons are produced from the decay of a Higgs boson) from background events 
(where two photons are produced by other SM processes).
The goal of the classifier is to assign high scores to signal-like events;
requirements can then be placed on the output score of the classifier 
to increase the S/B for the analysis categories.
The criteria for an event to be signal-like include having signal-like photon kinematics, 
and having high scores from the photon identification BDT.
In addition, events with a good diphoton mass resolution are preferred 
since this reduces the amount of background present under the signal peak.

To train the diphoton BDT, simulated events from the ggH, VBF, ttH 
and VH production modes are treated as signal. 
The event generator used for training the BDT (\textsc{POWHEG}) 
is different from that used to construct the final signal model.
The \textsc{POWHEG} sample is preferred for the BDT training because it does not contain
any events with negative weights.
It is checked that the input and output distributions from \textsc{POWHEG} agree with those
from \textsc{MadGraph5_{}aMC@NLO} within uncertainties.
Simulated background events include the contributions from prompt-prompt, prompt-fake, 
and fake-fake sources.
For both signal and background, 
events are weighted in accordance with the SM process cross section.
Furthermore, events from the QCD sample are down-weighted by a factor of 25;
the limited number of events in this sample results in very high per-event weights, 
which causes the classifier to over-estimate the importance of individual events.
All events are required to satisfy the analysis selection criteria.
The input variables to the classifier are the kinematic properties of the diphoton system, 
a per-event estimate of the diphoton mass resolution, 
and the photon identification scores of each photon.
The set of input variables is chosen such that the value of the Higgs boson mass cannot be inferred;
it is for this reason that the photon momenta are divided by the diphoton mass.
The full list of input variables is as follows:
\begin{itemize}
\item the transverse momentum of the two leading photons, divided by the diphoton mass, $\pt^{1,2}/\mgg$;
\item the pseudorapidity of the two leading photons, $\eta^{1,2}$;
\item the cosine of the angle between the two photons in the transverse plane, $\cos{(\Delta\phi)}$;
\item the output score of the photon identification BDT for the two leading photons;
\item the per-event relative mass resolution estimate, 
      under the hypothesis that the mass was computed using the correct primary vertex \srv/\mgg;
\item the per-event relative mass resolution estimate, 
      under the hypothesis that the mass was computed using an incorrect primary vertex \swv/\mgg;
\item the per-event probability estimate that the correct primary vertex was selected, $p_{vtx}$,
      which is the output of the vertex probability BDT described in Chapter~\ref{chap:objects};
\end{itemize}

The per-event relative mass resolution under the correct vertex hypothesis 
depends only on the photon energy measurements performed by the ECAL.
It is calculated by propagating the photon energy resolution estimates, 
assuming the resolutions are independent and Gaussian distributed:

\begin{equation}
  \srv/\mgg = \frac{1}{2} \sqrt{\left(\frac{\sigma_{E_1}}{E_1}\right)^{2}
                              + \left(\frac{\sigma_{E_2}}{E_2}\right)^{2}}
\end{equation}

where both the energy ($E_{1,2}$) and energy resolution ($\sigma_{E_{1,2}}$) estimates 
are obtained from the regression described in Chapter~\ref{chap:objects}.
Under the incorrect vertex hypothesis, an additional term must be added to account for the 
worsening in the diphoton mass resolution due to the uncertainty of the vertex position.
The distance between the correct and incorrect vertices is assumed to follow a Gaussian distribution
with a width dependent on the size of the beamspot.
The contribution from the vertex ($\sigma_{vtx}$) can then be computed analytically 
using the measured positions of the photons in the detector 
and combined in quadrature with \srv to calculate \swv:
%TODO understand this better

\begin{equation}
  \swv/\mgg = \sqrt{\left(\frac{\srv}{\mgg}\right)^{2}
                              + \left(\frac{\sigma_{vtx}}{\mgg}\right)^{2}}
\end{equation}

An example of the distribution for one of the input kinematic variables, the lead photon $\pt/\mgg$, 
is shown in Figure~\ref{fig:cat_ptominput}.
The background, split into its prompt--prompt, prompt--fake, and fake--fake components, 
is compared to the data sidebands (where the region with $115 < \mgg < \SI{135}{GeV}$ is excluded).
Good agreement between the two is observed.

\begin{figure}[hptb]
  \centering
  \includegraphics[width=0.49\textwidth]{Figures/Categorisation/leadptom_2016.pdf}
  \includegraphics[width=0.49\textwidth]{Figures/Categorisation/leadptom_2017.pdf}
  \caption[Leading photon scaled \pt distributions.]
  {
    Distribution of the leading photon $\pt/\mgg$ in background (stacked histogram) 
    and data (black points) events.
    The statistical uncertainty is shown by the orange band.
    The left plot shows 2016 data and MC,
    with 2017 data and MC on the right.
  }
  \label{fig:cat_ptominput}
\end{figure}

None of the input variables above encode the preference for events with good mass resolution.
For this purpose, an additional weight is applied to signal events which increases the relative 
importance of events with better values of the diphoton mass resolution. 
The weight applied, $w_{res}$, is given by the formula

\begin{equation}
  w_{res} = \frac{p_{vtx}}{\srv} + \frac{1-p_{vtx}}{\swv}
\end{equation}

This ensures that the classifier output score is higher for events 
which have a relatively low expected diphoton mass resolution.
Finally, the signal and background samples are divided into a training and a test set,
containing 70\% and 30\% of the total number of events respectively.
With this configuration of input variables, signal and background events, and event weights, 
the classifier is trained and its performance evaluated using the XGBoost software package~\cite{XGBoost}.

The performance of the training is evaluated using a receiver operating characteristic (ROC) curve, 
where the signal efficiency is plotted as a function of the background efficiency,
with each point corresponding to a specific threshold value placed on the classifier output score.
The area under the ROC curve is used to gauge the performance of a given classifier; 
a higher area corresponds to more effective discrimination between signal and background.
For the 2016 and 2017 trainings, the areas under the respective ROC curves are both equal to 0.85.

This metric is utilised to compare the performance of the classifier training 
with different values of the BDT's so-called hyper-parameters.
These hyper-parameters are parameters of the BDT, 
which are not learned but instead affect how the learning algorithm behaves.
The most important hyper-parameters affect the extent to which 
the algorithm learns the specific detail of the training sample provided.
To check for potential overtraining, the training score as measured by the area under the ROC curve 
of the training set is compared with the score from the independent test set.
The statistical variation of the ROC score can be estimated by varying 
the random seed for the training and by choosing a different subset of events for the training.
If the difference between the test and training set scores is significantly higher 
than this statistical variation, the classifier has been overtrained.
The default classifier hyper-parameters as defined 
in the XGBoost package~\cite{XGBoost} display no overtraining.
A coarse hyper-parameter optimisation procedure is then performed, 
in which each hyper-parameter is varied individually.
No significant improvement without overtraining is found, 
and therefore the default training parameters are used.

An additional parameter which can be optimised is the relative weight 
of the signal and background samples.
With the default sample weights corresponding to the SM sample cross section, 
the two classes are highly imbalanced.
This imbalance can cause suboptimal learning in the classifier.
To check this, the classifier is trained in a scenario 
where the signal event weights are increased by a uniform factor, 
such that the total sum of weights for signal and background is equal.
This is purely a technical change to the training 
designed to improve the learning outcomes of the classifier.
When evaluating the performance, the default event weights are used.
Equalising the total weights in this way results in a modest improvement in performance.
The areas under the ROC curves become 0.87 for both the 2016 and 2017 datasets.
The improvement is relatively small, 
but is however larger than the variation 
in training performance induced by either changing the random seed used for training or 
choosing different subsets of the input data for training and testing.
Therefore we choose to use this training scenario for the final classification.
The hyper-parameter optimisation procedure was repeated 
and no significant improvement was obtained.

%The ROC curves for the two different event weighting scenarios are shown in Figure~\ref{fig:cat_ROCs}.
%It is evident that the scenario with equalised weights has slightly superior performance.
%In addition, the relative importance of each feature is shown in Figure~\ref{fig:cat_importance}.
%This plot serves as validation that the classifier is learning 
%sensible features of the input dataset.
%TODO add ROC curve and feature importance plots here?

Validation of the diphoton BDT is performed using \Zee events, 
where simulated Drell-Yan events are compared with data.
This validation is important because although the background model used in the analysis 
is entirely data-driven, the signal model is taken from simulation.
Therefore it is necessary to ensure that there is reasonable agreement 
between data and simulation for signal-like objects,
for both the inputs to the diphoton BDT and the output score itself.
In this \Zee control region, the electrons are reconstructed as photons, 
and the presence of an electron pair with invariant mass $80 < m_{ee} < \SI{100}{GeV}$ is required.
Otherwise the event selection is the same as the analysis selection for diphoton events, 
except for the additional requirement that the leading electron $\pt > \SI{40}{GeV}$.
This requirement is necessary to ensure that no bias is introduced by the electron trigger, 
which has selection thresholds at $\pt = \SI{32}{GeV}~(\SI{35}{GeV})$ for 2016 (2017) data.

Figure~\ref{fig:cat_diphoBDT} shows the output score of the diphoton BDT for data and simulation.
The effect of two systematic uncertainties which affect the diphoton BDT are included;
these are the shift in the photon identification BDT score, 
and the uncertainty on the photon energy resolution (see Chapter~\ref{chap:sigbkg} for details).
Good agreement is observed between the data and simulation in both years, 
with all discrepancies covered by the statistical and systematic uncertainties.

\begin{figure}[hptb]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/Categorisation/DiphoBDT_2016.pdf}
  \includegraphics[width=0.7\textwidth]{Figures/Categorisation/DiphoBDT_2017.pdf}
  \caption[Validation of the diphoton BDT in \Zee events.]
  {
    Score of the diphoton BDT in $\Zee$
    events where the electrons are reconstructed as photons.
    The points show the score for data, the histogram shows
    the score for simulated Drell--Yan events, including statistical and 
    systematic uncertainties (pink band).
    The top plot shows 2016 data and MC,
    with 2017 data and MC on the bottom.
    Figures first shown in Ref.~\cite{HIG-18-029}.
  }
\label{fig:cat_diphoBDT}
\end{figure}

\subsection{Category definitions}

Once the diphoton BDT has been constructed, 
a category optimisation procedure can be performed for each Stage 1 bin independently.
The reconstructed number of jets and \ptgg define the category type into which a given event falls.
Then, independently for each category type, 
an optimisation procedure is performed to define diphoton BDT boundaries 
for a given number of subcategories.
The Approximate Mean Significance (AMS) is used to define the figure of merit for the optimisation.
The value of the AMS metric corresponds to the expected significance of a signal 
from the likelihood ratio statistic for a simple counting experiment, 
neglecting the impact of systematic uncertainties.
Its derivation is outlined in Ref.~\cite{Asymptotic}.
The formula for the AMS for a given analysis category is given by 

\begin{equation*}
  AMS = \sqrt{ 2 \left( (S+B) \ln{\left(1+\frac{S}{B}\right)} - S \right) }
\end{equation*}

where $S$ is 68\% (corresponding to $\pm 1\sigma_{eff}$) of the number of signal events 
from the desired truth bin (not the total number of signal events), and $B$ is the background.
The value of B is calculated by performing an exponential fit to the background, 
and then integrating the number of events between $125 - \sigma_{eff} < \mH < 125 + \sigma_{eff}$.
This formula reduces to $S/\sqrt{S+B}$ in the limit of small $S/B$.
Results are found to be robust to the choice of metric; 
only in bins with a very low number of events (e.g. the BSM bins) 
does the AMS metric return different optimal diphoton BDT boundaries from the $S/\sqrt{S+B}$ metric, 
and then the differences are relatively small.
The total AMS significance is computed by summing the values for each analysis category in quadrature.

The optimisation procedure itself is performed using a random search. 
This is found to be more computationally efficient than a grid search.
After the diphoton BDT boundaries have been established, 
a cross-check is performed to ensure that a sensible minimum has been found.
This is done by evaluating the AMS score for diphoton BDT values 
close to the nominal optimal point individually for each category type and each boundary.
If a slightly better performing point is found in the vicinity, 
it replaces the value returned by the random search.
The cross-check is designed to check that the random search has found a stable minimum; 
this is determined by examining the shape of the scan around the optimal point.
In general, the results of the random search are found to be robust.

This optimisation process is repeated for different numbers of categories.
No improvement beyond two categories per target STXS bin is observed, 
except for the 0J bin, which requires three categories.
The boundaries chosen and the expected number of signal and background events, 
together with the expected significance, 
are shown in Table~\ref{tab:cat_ggHsignificance2016} and Table~\ref{tab:cat_ggHsignificance2017} 
for 2016 and 2017 simulation and data respectively.
These tables illustrate the tendency for events with higher \ptH to have higher diphoton BDT scores.

\begin{table}
  \begin{centering}
    \input{Tables/ggHcats2016.tex}
    \caption[Definitions of 2016 categories targeting ggH production.]
    {
      The chosen diphoton BDT boundaries, 
      the expected number of signal (S) and background (B) events, 
      and the expected significance (defined by the AMS metric) of each category in the ggH phase space 
      for 2016 data and simulation, assuming an integrated luminosity of \SI{35.9}{\fbinv}.
      The categories targeting each stage 1 bin are ordered such that ``Tag 0" 
      has the highest S/B value, ``Tag 1" the next highest, and so on. 
    }
    \label{tab:cat_ggHsignificance2016}
  \end{centering}
\end{table}

\begin{table}
  \begin{centering}
    \input{Tables/ggHcats2017.tex}
    \caption[Definitions of 2017 categories targeting ggH production.]
    {
      The chosen diphoton BDT boundaries, 
      the expected number of signal (S) and background (B) events, 
      and the expected significance (defined by the AMS metric) of each category in the ggH phase space 
      for 2017 simulation and data, assuming an integrated luminosity of \SI{41.5}{\fbinv}.
      The categories targeting each stage 1 bin are ordered such that ``Tag 0" 
      has the highest S/B value, ``Tag 1" the next highest, and so on. 
    }
    \label{tab:cat_ggHsignificance2017}
  \end{centering}
\end{table}

\clearpage

\section{Vector boson fusion categorisation}
\subsection{Signal bin definitions}

The VBF process is divided into five particle level bins at stage 1 of the STXS framework.
Events where the Higgs boson is produced in association with a vector boson (VH, where V = W or Z) 
and the vector boson decays hadronically are included together with VBF.
In this region, there are two bins defined analogously to the VBF-like bins for ggH production, 
requiring a dijet with $m_{jj} > \SI{400}{GeV}$ and $\Delta\eta > 2.8$, 
split by a \SI{25}{GeV} boundary in \ptHjj.
In addition, there is a ``VH-like" bin, 
which requires the presence of a dijet with $60 < m_{jj} < \SI{120}{GeV}$. 
A ``BSM-like" bin is also defined, where the \pt of the leading jet is greater than \SI{200}{GeV}.
All remaining events fall into the ``VBF rest" bin.
Each bin is exclusive; all bins, except for the BSM bin, 
are required to have the leading jet $\pt < \SI{200}{GeV}$.
Table \ref{tab:cat_VBFfractions} shows a summary of the definition of each bin 
and the corresponding fraction of the total VBF cross section.
The inclusive VBF cross section is \SI{3.779}{pb} at $\mH = \SI{125.09}{GeV}$~\cite{YR4},
computed at approximately next-to-next-to-leading (NNLO) order in QCD and NLO in EW,
meaning some but not all NNLO diagrams are included in the calculation.
Of this approximately \SI{3.52}{pb} is within $|y_H| < 2.5$.
For hadronic VH production, 
the cross section at $\mH = \SI{125.09}{GeV}$ of \SI{1.54}{pb}~\cite{YR4}
is computed at NNLO in QCD and NLO in EW, 
with around \SI{1.37}{pb} within $|y_H| < 2.5$.

\begin{table}
  \begin{centering}
    \input{Tables/VBFbins.tex}
    \caption[Particle level definitions of the VBF stage 1 STXS bins.]
    {
      The particle level definition of each VBF stage 1 bin 
      and the corresponding fractional and absolute cross sections.
      The fractions reported are normalised relative to inclusive VBF or VH hadronic production, 
      whilst the cross sections are the sum of the VBF and VH hadronic values.
      The fractions are estimated from simulated VBF and hadronic VH \Hgg events 
      within the region $|y_H| < 2.5$.
      Details of the simulated samples can be found in Section~\ref{chap:objects}.
      Each bin is exclusive; all bins except the BSM bin 
      are required to have the leading jet $\pt < 200$ GeV.
    }
    \label{tab:cat_VBFfractions}
  \end{centering}
\end{table}

\subsection{Categorisation strategy}

Different categorisation scenarios are considered for the VBF process.
Ideally categories would be constructed to target each stage 1 bin.
However, the experimental sensitivity to the BSM-like, VH-like, and VBF rest bins is limited.
Care is therefore taken not reduce the sensitivity to inclusive VBF production when designing 
the categories for each of these categories.

The bins which can be most precisely measured are the 2J-like and 3J-like VBF bins.
In addition to migrations between the categories targeting each bin, 
there is substantial contamination from events with two jets arising from ggH production.
For this reason the dijet BDT is trained to discriminate between ggH and VBF events.
The categorisation is then performed using the detector level equivalents
of the quantities used to define the bins: \mjj, \ptHjj, and leading jet \pt.
The dijet and diphoton BDTs are subsequently used 
to reduce the respective number of ggH and background events entering the categories, 
thus increasing their sensitivity.

\subsection{Dijet BDT}

The dijet BDT is trained to discriminate VBF events from both background and ggH events.
This is made possible due to the distinctive signature of VBF events, 
which typically have two jets with large separation in pseudorapidity and high invariant mass.
Therefore the input variables to the dijet BDT consist primarily of jet-related variables.
In addition, the jets in VBF events originate from quarks, 
whereas ggH events typically originate from gluons.
This results in subtle differences in the internal structure of the jet objects, 
as discussed in Chapter~\ref{chap:hgcal}.
Adding the jet shape variables used there as inputs 
do not significantly improve the performance of the dijet BDT;
however it has been shown that more sophisticated techniques using neural network classifiers
and detailed jet inputs can improve the performance~\cite{JackThesis}.
The full set of input variables for the dijet BDT are:
\begin{itemize}
\item the transverse momentum of the two leading photons, divided by the diphoton mass, $\pt^{1,2}/\mgg$;
\item the transverse momentum of the two leading jets, $\pt^{j1,j2}$;
\item the invariant mass of the dijet, \mjj;
\item the magnitude of the difference in pseudorapidity of the two leading jets, $|\Delta\eta|$;
\item the magnitude of the difference in azimuthal angle 
      between the two leading jets, $|\Delta\phi_{jj}|$;
\item the magnitude of the difference in azimuthal angle 
      between the diphoton and dijet, $|\Delta\phi_{\gamma\gamma,jj}|$;
\item the minimum angular separation between either of the two leading photons 
      and either of the two leading jets, $\Delta R_{\textrm{min}}(\gamma,j)$;
\item the centrality variable, which is given by:
\begin{equation}
C_{\gamma\gamma} = \mathrm{exp}\left(-\frac{4}{(\eta_{j1} - \eta_{j2})^{2}}\left( \eta_{\gamma\gamma} - \frac{\eta_{j1} + \eta_{j2}}{2} \right)^{2}\right)
\end{equation}
where $\eta_{j1}$ and $\eta_{j2}$ are the pseudorapidities of the 
leading and subleading jets respectively.
\end{itemize}

In previous versions of the analysis \cite{HIG-16-040}, 
both the signal and background for the dijet BDT training have been taken directly from simulation. 
However there are a limited number of events in the prompt-fake and fake-fake background samples 
which pass the analysis preselection, 
and this problem is exacerbated by applying additional requirements on the dijet system.
%Have a go at explaining why here?
Consequently, there are few events with which to train the dijet BDT, 
and those events which are present have extremely high weights.
This results in multiple issues that reduce the effectiveness of the dijet BDT.
If these high-weight events are included in the training, 
the classifier assigns too much importance to specific instances 
and does not successfully learn generalised features of the input datasets.
Attempts have been made to mitigate this, 
the first of which is to reduce the weight of the QCD events by a factor of 25.
In addition, the VBF preselection is loosened in 
order to include more events in the training.
The standard VBF preselection consists of the analysis preselection 
with the additional requirements of $\pt > 40~(30)$ GeV for the leading (subleading) jet, 
$\mjj > \SI{250}{GeV}$ and photon identification BDT score of greater than -0.2 for both photons.
The loosened version reduces the \pt thresholds by \SI{10}{GeV} each, 
the \mjj threshold to \SI{100}{GeV}, and removes the tighter photon identification requirement.
This slightly improves the training performance; 
however the background rejection is still suboptimal 
due to the change in phase space and reduction in event weights.
Furthermore, this makes the simulation very difficult to validate against data
since the distributions are highly discontinuous.
An illustration of the effect is shown in Figure~\ref{fig:cat_mjjinput}, 
which shows the discontinuous \mjj distribution after the VBF preselection is applied.

\begin{figure}
  \centering
  \includegraphics[width=0.49\textwidth]{Figures/Categorisation/mjj_2016.pdf}
  \includegraphics[width=0.49\textwidth]{Figures/Categorisation/mjj_2017.pdf}
  \caption[Dijet invariant mass distributions.]
  {
    Distribution of the dijet invariant mass in background (stacked histogram) 
    and data (black points) events.
    The statistical uncertainty is shown by the orange band.
    The left plot shows 2016 data and MC,
    with 2017 data and MC on the right.
  }
  \label{fig:cat_mjjinput}
\end{figure}

An alternative solution to this problem is to use events from a data control region 
to replace the MC samples with low numbers of events.
In this way, the more abundant data events are used to replace the simulated prompt-fake and fake-fake 
events used to train the dijet BDT.
There are a sufficient number of prompt-prompt events in MC, 
which do not need replacement and continue to be taken from simulation for the training.
This method is used for the first time in this analysis;
the outline of the procedure is as follows:
\begin{itemize}
\item define a control region by inverting the photon identification BDT requirement 
      for events entering the VBF signal region;
\item use simulation to compute the relative fraction of events in the signal region and control region, 
      for each photon, as a function of the photon kinematics;
\item apply these factors as event weights to the data in the control region 
      and use these events to replace the simulation of prompt-fake and fake-fake events in the training.
\end{itemize}

There are four distinct regions considered in the data-driven replacement method.
Events entering the analysis comprise the signal region, 
where both photons pass the photon identification BDT requirements;
the total number of events is $N_{\textrm{pp}}$.
There are two control regions, where one photon passes the requirement and one fails it.
The number of events in each are denoted by $N_{\textrm{pf}}$ 
(where the leading photon passes and the subleading photon fails) and $N_{\textrm{fp}}$ 
(where the leading photon fails and the subleading photon passes).
Lastly there is the region where both photons fail the requirement, with $N_{\textrm{ff}}$ events.
%Table~\ref{tab:cat_RegionDefinitions} shows the definition of each region.
%TODO put in table. Mention reason for gap in ID BDT score?
Figure~\ref{fig:cat_DDschematic} illustrates how the method works.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{Figures/Categorisation/DDschematic.pdf}
  \caption[The data-driven method for dijet BDT training.]
  {
    A schematic illustrating the data-driven method for replacing the simulated
    prompt-fake and fake-fake events with reweighted data events from control regions 
    defined by the photon identification BDT. 
    The ratio of events which pass the photon identification requirement to events that fail 
    is taken from simulation in bins of \pt and $\eta$, 
    and used to calculate so-called ``fake-factors". 
    These fake factors are then applied to events in data where one or more photons fail 
    the photon identification requirement, 
    and these reweighted events replace the simulation in the training of the dijet BDT.
  }
  \label{fig:cat_DDschematic}
\end{figure}

Once these regions have been defined, it is possible to define so-called ``fake-factors" 
to extrapolate from the number of events in the control regions 
to the expected number of events in the signal region.
These factors are calculated from simulation, in bins of \pt and $\eta$; 
this ensures that the distributions of \pt and $\eta$ are well-modelled.
Kinematic variables other than \pt and $\eta$ are assumed to be similar 
across the control and signal regions.
This is checked by comparing the distributions constructed using the data-driven replacement 
method with data in the $\mgg$ sideband.
In order to minimise the statistical error on the fake factors, 
the loosened VBF preselection is used.
The full VBF preselection is then applied before training, 
once the events have been weighted by the fake factors.

The expression for the fake factor consists of the factor 
which extrapolates from the control region to the signal region, $w_{\textrm{fake}}$, 
multiplied by the fraction of events in the data which are either prompt-fake or fake-fake, 
$w_{\textrm{QCD}}$.
The second term is required since we wish to extract 
only the prompt-fake and fake-fake components from the data;
the prompt-prompt component is still taken from simulation.
Each term is applied to the photon which fails the identification requirement, 
and depends on the \pt and $\eta$ of this photon.
This can be written as:

\begin{equation*}
w_{\textrm{fake}} = w_{\textrm{fake}}(\eta,\pt)
= \left( \frac{N^{SR}} {N^{CR}} \right)_{MC} , 
\end{equation*}
\begin{equation*}
w_{\textrm{QCD}} = w_{\textrm{QCD}}(\eta,\pt)
= \left( \frac{N^{CR}_{\textrm{pf}} + N^{CR}_{\textrm{ff}}} {N^{CR}_{\textrm{pp}} + N^{CR}_{\textrm{pf}} + N^{CR}_{\textrm{ff}}} \right)_{MC} ,
\end{equation*}
\begin{equation*}
f = f(\eta,\pt) = w_{\textrm{fake}} \times w_{\textrm{QCD}} , 
\end{equation*}

where $N^{SR}$ and $N^{CR}$ are the number of events in the signal and control regions respectively;
each is a function of the photon \pt and $\eta$.
The number of prompt-prompt, prompt-fake, and fake-fake events in the control regions 
are also a function of photon \pt and $\eta$, 
and are denoted by $N^{CR}_{\textrm{pp}}$, $N^{CR}_{\textrm{pf}}$, and $N^{CR}_{\textrm{ff}}$ respectively.
The fake factors are calculated in four bins of \pt and two bins of $\eta$.
Figure~\ref{fig:cat_FakeFactors} shows the values of the factors in each of these bins.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{Figures/Categorisation/fakeFactors.png}
  \caption[Values of the fake factors used in the data-driven dijet BDT method.]
  {
    Fake factor values in each of the four \pt and two $\eta$ bins.
    The left two plots show the values where the subleading photon is a fake, 
    whilst in the right two plots the leading photon is fake.
    The values are taken from simulation.
  }
  \label{fig:cat_FakeFactors}
\end{figure}

%TODO include this??
%The final expression for calculating the number of events from the data-driven method 
%also takes into account the double-counting of the $N_{ff}$ contribution.

Validation of the data-driven method is performed to confirm that its output is 
a suitable replacement for the simulation.
The new training inputs, 
comprising the simulated prompt-prompt events and data-driven prompt-fake and fake-fake events, 
are compared with data from sidebands in the diphoton mass distribution.
The usual signal region selection is applied to the sideband data, 
but with the requirement that the diphoton mass does not lie in the region $115 < \mgg < \SI{135}{GeV}$.
These data cannot be used for training the dijet BDT since its reuse in the final fits 
could then induce bias; 
however it is very useful for validating the method, 
since its kinematic properties should be essentially identical to the data-driven output.
Figure~\ref{fig:cat_DDvalidation} shows the good agreement between the data-driven output
and the sideband data. 
The figure also shows how the method solves the problem of the high-weight events present in the MC; 
this confirms that it fulfils its purpose.

\begin{figure}
  \centering
  \includegraphics[width=0.49\textwidth]{Figures/Categorisation/DDvalidation_mjj2016.png}
  \includegraphics[width=0.49\textwidth]{Figures/Categorisation/DDvalidation_mjj2017.png}
  \caption[Validation of the data-driven method.]
  {
    Validation of the data-driven method. 
    The upper parts of the plots show the simulated background (blue histogram), 
    the background obtained with simulation for the prompt-prompt component 
    and utilising the data-driven method for the prompt-prompt and prompt-fake components 
    (green and grey stacked histograms),
    and mass sideband data (black points).
    The distributions of the ggH (yellow line) and VBF (red line) are also shown.
    Two ratio plots are also included, with one comparing the data-driven method to the sideband data, 
    and the other comparing the simulation with the sideband data.
    Data and simulation from 2016 are shown on the left, with 2017 on the right.
  }
  \label{fig:cat_DDvalidation}
\end{figure}

The dijet BDT is then trained with the data-driven inputs.
For the 2016 dataset, the area under the ROC curve is 0.87.
This constitutes a significant improvement over the score when the BDT is trained only with simulation, 
which is 0.84.
As an additional cross-check, the BDT is trained with events from the data sideband. 
The area of this ROC curve is also 0.87, 
which confirms that the observed improvement is reasonable.
The performance is 2017 is very similar, 
with the data-driven method improving the ROC score from 0.83 to 0.86.
After the category boundaries have been optimised, 
this results in an increase in the final value of the AMS score of approximately 10\%.

The agreement between data and simulation for the dijet BDT score is validated 
in a similar way to the diphoton BDT.
Events from the \Zee control region are required to pass the VBF preselection.
Figure~\ref{fig:cat_dijetBDT} shows the output score of the dijet BDT for data and simulation.
The systematic uncertainties included in the plot are those affecting the 
jet energy scale and the jet energy resolution corrections.
Good agreement is observed between the data and simulation in both years, 
with all discrepancies covered by the statistical and systematic uncertainties.

\begin{figure}[hptb]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/Categorisation/DijetBDT_2016.pdf}
  \includegraphics[width=0.7\textwidth]{Figures/Categorisation/DijetBDT_2017.pdf}
  \caption[Validation of the dijet BDT in \Zee events.]
  {
    Score of the dijet BDT in $\Zee$
    events where the electrons are reconstructed as photons.
    The points show the score for data, the histogram shows
    the score for simulated Drell--Yan events, including statistical and 
    systematic uncertainties (pink band).
    The top plot shows 2016 data and MC,
    with 2017 data and MC on the bottom.
    Figures first shown in Ref.~\cite{HIG-18-029}.
  }
  \label{fig:cat_dijetBDT}
\end{figure}

\subsection{Category definitions}

In previous versions of the analysis, 
the categories targeting VBF production were defined using the so-called ``combined BDT".
The combined BDT is designed to incorporate both the dijet and diphoton information 
in order to optimally construct the VBF categories.
Its input variables are:
\begin{itemize}
\item the diphoton BDT score
\item the dijet BDT score
\item the transverse momentum of the two leading photons, divided by the diphoton mass, $\pt^{1,2}/\mgg$;
\end{itemize}
The combined BDT is trained with VBF as signal against the three sources of SM background, 
using simulated events in each case.
Gluon fusion is not included in the training because it is found to worsen the performance 
of the combined BDT to reject non-Higgs boson background.
Since these backgrounds have a greater impact on the final sensitivity 
than the contamination from other signal processes, 
this is considered a higher priority.
However, it does mean that the ggH rejection of the combined BDT can be sub-optimal.
Here the efficacy of the combined BDT is compared with the more direct approach of 
defining the VBF categories by imposing thresholds on the diphoton and dijet BDT scores directly.

In addition to comparing the two different BDT approaches, 
different possible categorisation scenarios are considered.
In Ref.~\cite{HIG-16-040}, three inclusive categories are defined 
by placing thresholds on the output score of the combined BDT.
For this analysis, further splitting using kinematic variables is required 
if the different VBF stage 1 bins are to be measured individually.
%However it is found that optimising for each stage 1 bin independently, 
%considering only one stage 1 bin as signal in each case, 
%reduces the overall sensitivity to the VBF process.
%Therefore when comparing different categorisation scenarios,
When comparing different categorisation scenarios,
the figure of merit considered is the total VBF significance, 
computed using the AMS metric with all VBF bins considered as signal.
Scenarios with additional splits following the stage 1 bin definitions 
will then be considered preferable, 
provided they do not substantially reduce the overall VBF significance, 
since they enable individual stage 1 bins to be measured in addition to the overall VBF cross section.
In each scenario, there is a single dedicated VBF BSM category 
which requires that the \pt of the leading jet is greater than \SI{200}{GeV}.
The list of scenarios considered is as follows:
\begin{itemize}
\item \textbf{Inclusive two category:} Two categories are considered, 
      with no additional kinematic selection criteria applied.
      Including the VBF BSM category, this gives three categories in total.
\item \textbf{Inclusive three category:} Three categories are considered, 
      with no additional kinematic selection criteria applied.
      It is almost equivalent to the approach in Ref.~\cite{HIG-16-040}, 
      aside from the additional VBF BSM category.
      This gives a total of four categories.
\item \textbf{Split by \ptHjj:} Two categories are considered 
      for each of the 2J-like and 3J-like VBF bins,
      using a boundary at \SI{25}{GeV} on the reconstructed value of \ptHjj.
      This gives a total of five categories.
\item \textbf{Split by \ptHjj and \mjj:} Two categories are considered
      for each of the 2J-like and 3J-like VBF bins,
      using a boundary at \SI{25}{GeV} on the reconstructed value of \ptHjj.
      Additionally, the requirement that \mjj is greater than \SI{400}{GeV} is applied.
      A fifth category targeting principally the ``VBF rest" bin, with $250 < \mjj < 400$,
      is therefore included.
      This gives a total of six categories.
\end{itemize}

A boundary optimisation procedure is followed for each scenario, 
using the same methodology as for the ggH categories.
For the approach which uses the diphoton and dijet BDT output scores directly, 
the values of the thresholds on the two BDTs are optimised simultaneously for each category.
The resulting significance values of each scenario for the 2016 and 2017 datasets 
are shown in Tables~\ref{tab:cat_VBFscenarios2016} and \ref{tab:cat_VBFscenarios2017} respectively.
The performance of the combined BDT approach is compared with the approach using the diphoton 
and dijet BDT scores directly in each case.
For both years, the combined BDT performs slightly worse 
than the direct use of the diphoton and dijet BDTs.
This can be attributed to the fact that the combined BDT is trained only on simulation; 
it is not possible to use the data-driven method for its training 
since the photon information is used as inputs.
Furthermore, the results demonstrate that there is no substantial reduction 
in the overall VBF significance with the most granular categorisation scenario.
It is therefore considered the best scenario for facilitating measurements of individual stage 1 bins, 
and is chosen as the VBF categorisation scheme for the analysis.

Tables~\ref{tab:cat_VBFcuts2016} and \ref{tab:cat_VBFcuts2017} show the final thresholds 
for the diphoton and dijet BDTs in each category, 
together with the expected number of signal and background events and the per-category significance.

\begin{table}
  \begin{centering}
    \input{Tables/VBFcomparison2016.tex}
    \caption[Comparison of 2016 VBF categorisation scenarios.]
    {
      The total VBF significance (defined by the AMS metric) for different categorisation scenarios 
      using 2016 data and simulation, assuming an integrated luminosity of \SI{35.9}{\fbinv}.
      Classification using the combined BDT is compared with setting boundaries with the 
      diphoton and dijet BDTs directly.
    }
    \label{tab:cat_VBFscenarios2016}
  \end{centering}
\end{table}

\begin{table}
  \begin{centering}
    \input{Tables/VBFcomparison2017.tex}
    \caption[Comparison of 2017 VBF categorisation scenarios.]
    {
      The total VBF significance (defined by the AMS metric) for different categorisation scenarios 
      using 2017 data and simulation, assuming an integrated luminosity of \SI{41.5}{\fbinv}.
      Classification using the combined BDT is compared with setting boundaries with the 
      diphoton and dijet BDTs directly.
    }
    \label{tab:cat_VBFscenarios2017}
  \end{centering}
\end{table}

\clearpage

\begin{table}
  \begin{centering}
    \input{Tables/VBFcats2016.tex}
    \caption[Definitions of 2016 categories targeting VBF production.]
    {
      The chosen diphoton and dijet BDT boundaries, 
      the expected number of signal (S) and background (B) events, 
      and the expected significance (defined by the AMS metric) of each category in the VBF phase space 
      for 2016 data and simulation, assuming an integrated luminosity of \SI{35.9}{\fbinv}.
      The categories targeting each stage 1 bin are ordered such that ``Tag 0" 
      has the highest S/B value, ``Tag 1" the next highest, and so on. 
    }
    \label{tab:cat_VBFcuts2016}
  \end{centering}
\end{table}

\begin{table}
  \begin{centering}
    \input{Tables/VBFcats2017.tex}
    \caption[Definitions of 2017 categories targeting VBF production.]
    {
      The chosen diphoton and dijet BDT boundaries, 
      the expected number of signal (S) and background (B) events, 
      and the expected significance (defined by the AMS metric) of each category in the VBF phase space 
      for 2017 data and simulation, assuming an integrated luminosity of \SI{41.5}{\fbinv}.
      The categories targeting each stage 1 bin are ordered such that ``Tag 0" 
      has the highest S/B value, ``Tag 1" the next highest, and so on. 
    }
    \label{tab:cat_VBFcuts2017}
  \end{centering}
\end{table}

\clearpage

\section{Summary}

A set of \Hgg analysis categories are defined in order to maximise the overall sensitivity 
of the analysis and to enable the measurement of different signal bins.
Categories targeting nine different subdivisions of the ggH production mode are constructed, 
using the reconstructed \ptgg and number of jets to infer the most likely signal bin.
The diphoton BDT is then used to further split these events into categories of differing S/B, 
which increases the precision of the measurement of each bin.
Furthermore, categories targeting the different VBF signal bins are constructed.
A dijet BDT is trained using a novel data-driven approach, 
and is used to discriminate against both ggH production and background processes.
It is used together with the diphoton BDT and the reconstructed values of \mjj, \ptHjj 
and the number of jets to define the final VBF analysis categories.
