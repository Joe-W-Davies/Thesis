\chapter{Measuring Higgs boson properties in the \Hgg decay channel}
\label{chap:hgg_overview}

\section{Introduction}
The following three chapters provide a detailed description of the CMS \Hgg analysis documented in Ref.~\cite{CMS-PAS-HIG-19-015}. This analysis measures Higgs boson properties in the diphoton decay channel, using proton-proton collision data at $\sqrt{s}$~=~13~TeV collected by the CMS experiment at the LHC from 2016 to 2018, corresponding to an integrated luminosity of 137~\fbinv. The increased statistical power, which comes with using the full Run 2 data, leads to an improvement in the precision of existing measurements and enables measurements of more granular regions of Higgs boson production phase space. 

Building upon the strategies developed in previous CMS \Hgg analyses~\cite{}, a set of orthogonal event categories are constructed to target bins at stage 1.2 of the STXS framework~\cite{}. The production cross sections are then measured directly from the diphoton invariant mass distributions in the event categories. This extremely powerful observable effectively distinguishes signal from background, where photon pairs produced via Higgs boson decay form a narrow signal peak centred around $m_H$, lying above a smoothly-falling background distribution from other SM processes. For events with two reconstructed photons, $\gamma_1$ and $\gamma_2$, the diphoton invariant mass, $m_{\gamma\gamma}$, is calculated according to,

\begin{equation}\label{eq:mgg}
    m_{\gamma\gamma} = \sqrt{2E_{\gamma1}E_{\gamma2}(1-\cos{\theta})},
\end{equation}

\noindent
where $E_{\gamma1}$ and $E_{\gamma2}$ are the measured energies of $\gamma_1$ and $\gamma_2$ respectively, and $\theta$ is the opening angle between the two photons. As displayed graphically in Figure~\ref{fig:hgg_overview_improving_measurements}, it is possible to isolate three aspects of the $m_{\gamma\gamma}$ spectrum which affect the sensitivity to Higgs boson properties: 

\begin{enumerate}
    \item The diphoton mass resolution: a narrower signal peak leads to enhanced sensitivity. This can be improved by correctly identifying photons from other objects in the detector such as jets, and subsequently making accurate measurements of the photon position and energy. Furthermore, the precise assertion of the interaction vertex from which the two photons originate is crucial for accurately determining the opening angle, $\theta$.
    
    \item The signal-to-background ratio in an event category: the sensitivity is improved by reducing the background contamination under the Higgs boson peak. In this analysis, traditional cut-based methods are improved upon by using machine learning algorithms, which better discriminate between signal and background. Since different background processes are important for different Higgs boson production modes, a number of signal-vs-background classifiers are trained and used in the relevant event categories.
    
    \item The purity of the signal events in an event category: here, event categories are defined to target specific bins of the STXS framework. Therefore to improve the sensitivity, we aim to maximise the purity of an event category with respect to the targeted bin. In other words, make the confusion matrix between the truth-level STXS bin and the reconstruction-level event category as diagonal as possible (see Figure~\ref{fig:purity_matrix}). This in turn reduces the correlations between the measured cross sections.
\end{enumerate}

\begin{figure}[hptb]
  \centering
  \includegraphics[width=1\textwidth]{Figures/hgg_overview/hgg_improve_measurement.pdf}
  \caption[Avenues for improving \Hgg measurements.]
  {
    Detailed caption
  }
  \label{fig:hgg_overview_improving_measurements}
\end{figure}

This chapter will focus on the experimental techniques used to maximise the sensitivity of the analysis, according to the three aspects stated above. After first introducing the data and simulation samples used in the analysis, section \ref{sec:event_reconstruction} details the procedure used to reconstruct candidate \Hgg events, specifically the techniques which improve the diphoton mass resolution: the photon identification and selection, the photon energy regression and the primary vertex selection. Following this, the categorisation of events is described in section~\ref{sec:event_categorisation}. Additional objects in the events such as jets, charged leptons and missing transverse momentum are used to define categories targeting the different STXS stage 1.2 bins. This section covers the methods used to both reduce background contamination and to increase the purity of the targeted signal process in such categories.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Samples}
\subsection{Data}
This analysis uses p-p collision data collected by the CMS experiment at $\sqrt{s}$~=~13~TeV during Run 2 of the LHC. The total integrated luminosity is 137~\fbinv, of which 35.9~\fbinv was collected in 2016, 41.5~\fbinv in 2017 and 59.4~\fbinv in 2018.

Events in data are selected using the two-tiered trigger system described in section \ref{sec:trigger}. A seed at the L1T stage is defined as a deposit of energy in the ECAL, above a certain energy threshold, which is subsequently passed to the HLT path. Despite signal events being characterised by two photon candidates, a higher overall efficiency is achieved if only one seed is required at the L1T stage, with a tight transverse energy threshold applied to the seed in order to limit the rate of events passing to a manageable level. The energy threshold is typically set at 40~GeV, lowering to 30-32~GeV for isolated L1T seeds in the ECAL. With the presence of this energy threshold, there is an unavoidable drop in efficiency for \Hgg events at low transverse energy. To circumvent this effect, an additional double-seed selection is included at the L1T with lower energy thresholds than the single-seed trigger. In 2016, the thresholds were set to 23 and 10~GeV for the leading and subleading seeds respectively, rising to 25 and 14~GeV for the 2017 and 2018 data taking periods.

At the HLT, a basic version of the offline clustering algorithm, described in section \ref{sec:particle_flow}, is applied to the energy deposits in the ECAL. Here, events are required to contain two SCs with invariant mass greater than 90~GeV and passing asymmetric $p_T$ thresholds, initially set at 30 and 18~GeV. After the 2016 data-taking period, the lower threshold was raised to 22~GeV to counterbalance the increased instantaneous luminosity and hence maintain a constant event rate. In addition, a number of selection criteria are imposed on higher-level variables related to the SC shower shape, isolation and the ratio of the HCAL and ECAL deposits. Events passing the HLT selection then enter the \Hgg analysis.

The tag-and-probe method is used to evaluate the efficiency of the trigger selection~\cite{CMS:2011aa}. This method exploits the decay of a known resonance such as the $J/\psi$ or Z boson, where the \textit{tag} is defined as one of the decay products passing very tight identification criteria, and the \textit{probe} as the other decay product, subject to much looser identification requirements. Moreover, the combined invariant mass of the tag-and-probe pair is required to be consistent with the mass of the original resonance to ensure a high purity sample. For some selection criteria, $\mathcal{C}$, the efficiency, $\epsilon_{\mathcal{C}}$, is then defined as the fraction of probes passing $\mathcal{C}$. This method remains valid as long as the identification requirements on the probe do not affect the efficiency of $\mathcal{C}$.

Given the proximity of the Z boson and Higgs boson masses, as well as the fact that both electrons and photons are reconstructed as SCs in the ECAL, \Zee events in data provide an excellent candidate for evaluating efficiencies in the \Hgg analysis. Dielectrons with invariant mass close to the Z boson mass are used to define the tag and probe. After reweighting the \Zee events to match the $\eta$ and $R_9$ (see section \ref{sec:photon_preselection}) distributions of \Hgg events, the trigger efficiency is evaluated per SC in bins of the probe electron $p_T$, $\eta$ and $R_9$. For photons in the EB and EE, the trigger efficiency is above XX and YY\% respectively. The product of the two per-SC efficiencies is then used to weight simulated events to replicate the trigger efficiency observed in data.

\subsection{Simulation}
Monte Carlo (MC) simulated events are used for both training event classifiers and constructing the final signal model. 

Signal samples are simulated for the different Higgs boson production mechanisms at next-to-leading order (NLO) in quantum chromodynamics (QCD) using the \textsc{MG5\_aMC@NLO} (version 2.4.2)~\cite{}, and \textsc{Powheg} (version 2.0) generators~\cite{}. When possible, an independent event sample from the alternative generator is used for training event classifiers, thus ensuring the event categorisation and the construction of the final signal model are independent. Events produced via ggH production are weighted as a function of $p_T$ and $\eta$ to match the predictions of the NNLOPS program~\cite{}. Parton distribution functions (PDFs) are used to model the distribution of colliding partons inside the initial state protons. Events are subsequently interfaced with \textsc{Pythia8} (version 8.226 for 2016 MC and version 8.230 for 2017/2018 MC) for decaying the Higgs boson to photons, parton showering and hadronization~\cite{}. 

The signal samples are normalised according to the production cross sections and the \Hgg branching fraction (0.227\%) recommendations by the LHCHWG~\cite{}. The fractional breakdown of each production mode in the STXS stage 1.2 bins is computed directly from the signal MC samples, and serves as the SM prediction of the cross section in each bin. Table \ref{tab:signal_samples} summarises the event generators, PDF sets and \textsc{Pythia} tunes used for the signal simulation, as well as the total cross section times branching ratio for each production mode with details on the order of the calculation. The fractional breakdowns of each production mode into the stage 1.2 bins are shown in Tables~\ref{tab:stxs_frac_ggH}--\ref{tab:stxs_frac_top}.

\begin{table}[htb]
    \caption{Add caption: cross section evaluated at 13 TeV + MH. Actually fill table in correctly}
    \label{tab:signal samples}
    \vspace{.5cm}
    \centering
    \centering
    \scriptsize
    \renewcommand{\arraystretch}{1.8}
    \begin{tabular}{lccccc}
       & Generator (for classifiers) & \textsc{PYTHIA8} tune & PDF & $\sigma\cdot\rm{BR}$~[fb] & Order of $\sigma$ calc \\ \hline
       ggH & \textsc{Madgraph5\_aMC@NLO} & \textsc{Pythia8} & - & 110.27 & N$^{\rm{3}}$LO(QCD)+NLO(EW) \\
       gg$\rightarrow$ZH, Z$\rightarrow$qq & \textsc{Powheg} & \textsc{Pythia8} & - & 0.19 & NNLO(QCD)+NLO(EW) \\
       \hline
       VBF & \textsc{Madgraph5\_aMC@NLO} & \textsc{Pythia8} & - & 8.59 & NNLO(QCD)+NLO(EW) \\
       qq/qg$\rightarrow$WH, W$\rightarrow$qq & \textsc{Madgraph5\_aMC@NLO} & \textsc{Pythia8} & - & 2.10 & NNLO(QCD)+NLO(EW) \\
       qq/qg$\rightarrow$ZH, Z$\rightarrow$qq & \textsc{Madgraph5\_aMC@NLO} & \textsc{Pythia8} & - & 1.40 & NNLO(QCD)+NLO(EW) \\
       \hline
       qq/qg$\rightarrow$WH, W$\rightarrow\ell\nu$ & \textsc{Madgraph5\_aMC@NLO} & \textsc{Pythia8} & - & 1.016 & NNLO(QCD)+NLO(EW) \\
       qq/qg$\rightarrow$ZH, Z$\rightarrow\ell\ell/\nu\nu$ & \textsc{Madgraph5\_aMC@NLO} & \textsc{Pythia8} & - & 0.520 & NNLO(QCD)+NLO(EW) \\
       gg$\rightarrow$ZH, Z$\rightarrow\ell\ell/\nu\nu$ & \textsc{Powheg} & \textsc{Pythia8} & - & 0.084 & NNLO(QCD)+NLO(EW) \\
       \hline
       ttH & \textsc{MG5\_aMC@NLO} & \textsc{Pythia8} & - & 1.155 & NLO(QCD)+NLO(EW) \\
       \hline
       tHq & \textsc{MG5\_aMC@NLO} & \textsc{Pythia8} & - & 0.175 & NLO(QCD) in 5FS \\
       tHW & \textsc{MG5\_aMC@NLO} & \textsc{Pythia8} & - & 0.034 & NLO(QCD) in 5FS \\
       \hline
       bbH & \textsc{MG5\_aMC@NLO} & \textsc{Pythia8} & - & 1.108 & NNLO(5FS)+NLO(4FS) \\

    \end{tabular}
\end{table}

\begin{itemize}
    \item In the caption for STXS splitting tables, make clear definition of jets i.e. distance param etc.
\end{itemize}

The final background model used for the extraction of results is derived directly from data. Nevertheless, simulated background events are required for training the multivariate event classifiers. For inclusive production, the dominant source of background is SM diphoton production, which is simulated using the \textsc{sherpa} (version 2.2.4) generator~\cite{}. In this sample, matrix elements are calculated at NLO and LO for up to one and up to three additional partons respectively, which are subsequently matched with the \textsc{Sherpa} generator parton showering. A subdominant background originates from $\gamma$~+~jet or jet~+~jet events, where the jets are misidentified as photons. These backgrounds are simulated with \textsc{Pythia8}, applying a filter in the generation to enrich the production of jets with high electromagnetic activity. Furthermore, other sources of background become important for categories targeting the sub-dominant production modes, such as $V+\gamma\gamma$ in the VH leptonic categories and $tt+\gamma\gamma$ in the top-associated categories. Additional MC samples simulated with the XYZ are used to model such backgrounds.

Finally, Drell-Yan events with leptonic final states are used for validation purposes and are simulated with the \textsc{MG5\_aMC@NLO} generator. Plus others e.g. ttZ.

Each particle-level sample is propagated through the \textsc{Geant4} package to model the response of the CMS detector~\cite{}. Separate MC samples are produced for each year to account for the variations in the detector conditions and the LHC beam parameters. This modelling includes the effect of pileup interactions originating from both the nominal bunch cross-crossing (in-time pileup) and the crossing of previous and subsequent bunch crossings (out-of-time pileup). The simulation is weighted to match the distribution of the number of interaction vertices in data, which corresponds to an average pileup of 23 in 2016, and 32 in 2017 and 2018.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Event reconstruction}\label{sec:event_reconstruction}
This section describes the offline reconstruction of events passing the trigger selection. Add something here to be clear on reconstruction done on each year independently to account for changes in the detector performance and LHC beam parameters (don't overlap with previous paragraph too much).

\subsection{Photon reconstruction}\label{sec:photon_reconstruction}
Photons are defined using the set of photon candidates from the PF algorithm (see section \ref{sec:particle_flow}). In the algorithm, SC's are formed by clustering together deposits of energy in the ECAL crystals, consistent with originating from the same electromagnetic shower. Due to imperfect shower containment in the crystals and shower losses for photons which convert to \ee pairs before reaching the ECAL, the SC energy, \Eraw, can often differ from the true initial photon energy, \Etrue. A multivariate regression technique is used to correct for such losses per photon, estimating both \Etrue and its uncertainty. Remaining differences in the photon energy scale and resolution are accounted for using a series of additional scale and smearing corrections, derived using \Zee events. Photons are then subject to a set of selection criteria (pre-selection) concerning the photon shower shape, kinematic and isolation variables. In addition, a photon identification Boosted Decision Tree (BDT) is trained to separate genuine photons from fake photons. The selection includes a minimum requirement on the output score of this BDT to vastly reduce the contribution from background processes entering the analysis, in which hadronic jets mimic the photon signature. Before describing the photon reconstruction techniques in more detail, it is useful to list the photon variables used in \Hgg analysis.

\subsubsection{Photon variables}

\subsubsection{Photon energy}
The photon energy response of the ECAL (\Etrue/\Eraw) is parameterised by a function with a Gaussian core and power law tails. Using simulated photons, a multivariate regressor is trained to estimate the shape parameters of this energy response function, thus providing a prediction of the full \Etrue/\Eraw probability density function for each photon~\cite{}. The mode of this predicted function is used to correct \Eraw to \Etrue, whilst the shape provides a per-photon energy resolution estimate used later in the event categorisation (section \ref{sec:event_categorisation}).

The regressor aims to correct not only for the imperfect shower containment arising from converting photons and electromagnetic showers that begin upstream of the ECAL, but also for the localised containment within the ECAL, where energy can be lost in the gaps between crystals. Input variables related to the SC shower shape provide information on the upstream showering and photon conversions, which combined with the SC $\eta$ and $\phi$ values allows the regressor to learn variations in the ECAL geometry. On the other hand, the seed crystal positions and seed cluster energy ratios enable the regressor to correct for the localised containment effects. In addition, the total number of primary vertices and the total energy density, $\rho$, are included to account for systematic enhancements of \Eraw due to pileup.

After the regression, a number of residual discrepancies between data and simulation remain, that cannot be derived from simulation alone. Using \Zee events in data, in which the electrons are reconstructed as photons, a series of scale and smearing corrections are derived to correct the photon energy scale and resolution~\cite{}. To account for the degradation of the ECAL crystal performance over time, and the subsequent drift in energy scale this would cause, a time-dependent correction is applied in bins which equate roughly to the duration of one LHC fill. This ensures the energy scale of the ECAL is constant over time. Subsequently, the dielectron mass spectrum, $m_{ee}$, is used to simultaneously shift the peak in data to match that in simulation, and smear the resolution in simulation to match that in data. These scale and smearing corrections are applied differentially in bins of SC $\eta$ and \RNINE. 

Referring back to equation \ref{eq:mgg}, the techniques described above aim to minimize the photon energy resolution, $\sigma_E/E$, in turn minimizing the diphoton mass resolution, $\sigma_m/m_{\gamma\gamma}$, and thus leading to an enhanced sensitivity to Higgs boson production. Figure \ref{fig:photon_energy_0} shows the $m_{ee}$ distributions for \Zee events in both data and simulation, after the full set of energy corrections are applied. The left-hand panel shows the situation where both electrons are reconstructed in the ECAL barrel, and the right-hand panel where both are reconstructed in the ECAL endcaps. Clearly, after the corrections are applied, the data and simulation are in excellent agreement, well within the uncertainties.

\begin{figure}[hptb]
  \centering
  \includegraphics[width=1\textwidth]{Figures/hgg_overview/photon_energy_0.pdf}
  \caption[Dielectron mass spectrum for \Zee events in data and simulation after the energy corrections are applied]
  {
    Comparison of the dielectron mass spectrum for \Zee events in data (black points) and simulation (filled histogram), after the full set of energy scale and smearing corrections are applied. The total uncertainty on simulated events is shown by the red bands. The plots show the full data set collected during run II of the LHC, and the corresponding simulation, where the left-hand (right-hand) panel corresponds to events in which both electrons are reconstructed in the ECAL barrel (endcaps).
  }
  \label{fig:photon_energy_0}
\end{figure}

\subsubsection{Photon pre-selection}\label{sec:photon_preselection}
Table \ref{tab:photon_preselection} provides a schema of the selection criteria applied to the photon candidates. The pre-selection efficiency is calculated using the tag-and-probe method on \Zee events for all selection criteria, barring the requirement on the electron veto, which is computed using the photon in \Zmumug events. These efficiencies are computed for both data and simulation in bins of $\eta$ and \RNINE, and are typically above 95\% for photons with high values of \RNINE ($>0.85$) and around 90\% for photons with lower \RNINE ($<0.85$). The pre-selection efficiencies in simulation are scaled to match those in data.

\begin{table}[htb]
    \caption[Schema of the photon pre-selection criteria]{Schema of the photon pre-selection criteria. The shower shape and isolation requirements are different for photons in the ECAL barrel and for photons in the ECAL endcaps. These are then split into regions of different \RNINE criteria, with varying levels of additional selection on $\sigma_{\eta\eta}$, $\mathcal{I}_{\rm{ph}}$ and $\mathcal{I}_{\rm{tk}}$.}
    \label{tab:photon_preselection}
    \vspace{.5cm}
    \centering
    \scriptsize
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{p{0.25\textwidth}|p{0.7\textwidth}}
       Minimum \pt  & $p_T^{\gamma 1}>35$~GeV (leading), $p_T^{\gamma 2}>25$~GeV (subleading) \\
       $\downarrow$ & \\
       Geometrical acceptance & $|\eta| < 2.5$, excluding barrel-endcap transition region $1.44 < |\eta| < 1.57$ \\
       $\downarrow$ & \\
       Electron veto & True \\
       $\downarrow$ & \\
       Hadronic shower rejection & H/E~$<0.08$ \\
       $\downarrow$ & \\
       Shower shape and isolation requirements
       & For photons in the EB:
       \begin{tabular}{c|ccc}
            \RNINE & $\sigma_{\eta\eta}$ & $\mathcal{I}_{\rm{ph}}$ (GeV) & $\mathcal{I}_{\rm{tk}}$ (GeV) \\ \hline
            $>0.85$ & - & - & - \\
            $[0.50,0.85]$ & $<0.015$ & $<4.0$ & $<6.0$ \\
       \end{tabular} \\
       & \\
       & For photons in the EE:
       \begin{tabular}{c|ccc}
            \RNINE & $\sigma_{\eta\eta}$ & $\mathcal{I}_{\rm{ph}}$ (GeV) & $\mathcal{I}_{\rm{tk}}$ (GeV) \\ \hline
            $>0.90$ & - & - & - \\
            $[0.80,0.90]$ & $<0.035$ & $<4.0$ & $<6.0$ \\
       \end{tabular} \\
       & \\
       & All photons: (\RNINE$>0.8$ and $\mathcal{I}_{\rm{ch}}<20$~GeV) or ($\mathcal{I}_{\rm{ch}}/p_T^{\gamma}<0.3$)

    \end{tabular}
\end{table}


\subsubsection{Photon identification}
The photon identification BDT aims to distinguish between real photons in the CMS detector and hadronic jets mimicking a photon signature. The BDT is trained using the $\gamma$~+~jet simulation sample, where the true photon is used as signal and the fake photon from the jet as background. Photon shower shape, kinematic, and isolation variables are used as inputs to the BDT, along with parameters sensitive to pileup, such as $\rho$. 

One of the dominant sources of systematic uncertainty in the \Hgg analysis arises from the modeling of the electromagnetic shower in simulation, in particular the variables describing the shower shape and isolation. Since these variables are direct inputs to the photon ID BDT, any discrepancies between data and simulation are propagated to the output BDT score, and thus introduce a systematic uncertainty into the analysis. To mitigate this, a chained quantile regression (CQR) method is applied, which sequentially corrects the photon ID BDT input variables in simulation. 

The corrections are derived using an unbiased set of probe electrons in \Zee events. The CQR method aims to morph the distribution of some input variable, $y_i$, in simulation to match that in data by training a series of 21 BDTs to predict points along the cumulative distribution function, CDF($y_i$): [0.01,0.05,0.10,...,0.95,0.99]. The full CDF is determined by linearly interpolating between these 21 points, and is extracted separately for both data and simulation. A correction is then applied to the simulation, per electron, according to:
\begin{equation}
    y_i \longrightarrow y_i^{\rm{corr}} = \rm{CDF}^{-1}_{\rm{data}}(\rm{CDF}_{\rm{simulation}}(y_i)),
\end{equation}
\noindent 
which successfully morphs the distribution of $y_i$ in simulation to match that in data. 

Shower shape variables are first ordered into a chain: [$S_1$,$S_2$,...$S_N$]. The CDF for the first variable, $S_1$, is predicted using solely the electron $p_T$, $\eta$, $\phi$ and global energy density, $\rho$, as input features to the 21 BDTs. For variable, $S_i$, the input features also include the previously processed shower shape variables: [$S^{\rm{corr}}_1$,...,$S^{\rm{corr}}_{i-1}$] for simulation and [$S_1$,...,$S_{i-1}$] for data. By deriving the corrections in this manner, correlations between photon ID input variables are accounted for. The ordering of the chain is optimised to minimise the final discrepancy between data and simulation in the output photon ID BDT score.

For isolation variables, $\mathcal{I}_i$, an additional stochastic shifting procedure is applied to account for electrons migrating across the discontinuity in the distributions i.e. from the peak at $\mathcal{I}=0$, to the tail at $\mathcal{I}>0$. Two additional classifiers are trained to predict the probabilities for electrons to fall in the peak or tail, given the electron $p_T$, $\eta$ and $\phi$, and the energy density, $\rho$. Using the output probabilities, the simulated electrons are then migrated between the peak and tail to match the relative compositions in data. Finally, the distribution of electrons in the tail is then corrected using the quantile morphing technique described above. Figure \ref{fig:photon_id_0} demonstrates the performance of the CQR method for shower shape variable, \RNINE (left) and isolation variable, $\mathcal{I}_{\rm{ph}}$ (right).

\begin{figure}[hptb]
  \centering
  \includegraphics[width=.49\textwidth]{Figures/hgg_overview/dataMC_probeR9_0.pdf}
  \includegraphics[width=.49\textwidth]{Figures/hgg_overview/dataMC_probePhoIso_0.pdf}
  \caption[Corrections from the chained quantile regression method for \RNINE and $\mathcal{I}_{\rm{ph}}$]
  {
    Update plots to be neater!
  }
  \label{fig:photon_id_0}
\end{figure}

The systematic uncertainty originating from these corrections is derived by splitting the original \Zee samples in half, and re-calculating the corrections using the two independent event sets. The magnitude of the uncertainty is defined per bin of the output photon ID BDT score, as the standard deviation of the event-by-event corrected output score values. This assumes the major source of uncertainty in the method is originating from the limited size of the training samples. All in all, the CQR method provides a vastly improved technique for calculating the shower shape and isolation corrections, reducing the impact of a dominant systematic uncertainty from around 5\% in previous \Hgg analyses to roughly 2\%.

The output score of the photon ID BDT, for the lowest scoring photon in the photon pair, is shown in Figure \ref{fig:photon_id_1}. The left-hand plot shows the distribution for signal and background processes, highlighting the impressive discriminating power between events with two real photons and the $\gamma$~+~jet and jet~+~jet backgrounds. The right-hand plot shows the output score distribution for \Zee events in both data and simulation. The two are in excellent agreement, within the calculated uncertainties, demonstrating the impressive power of the CQR method in correcting the shower shape and isolation variables in simulation. 

Each photon in an event is required to have photon ID BDT score $>-0.9$. Akin to the pre-selection criteria, the efficiency of this requirement is derived in simulation and data using the tag-and-probe method on \Zee events, and the events in simulation are scaled to match the efficiency in data. In addition, the photon ID BDT score is used in numerous ways in the event categorisation.

\begin{figure}[hptb]
  \centering
  \includegraphics[width=1\textwidth]{Figures/hgg_overview/photon_id_0.pdf}
  \caption[Photon ID output score distributions]
  {
    Add caption
  }
  \label{fig:photon_id_1}
\end{figure}

\subsection{Vertex selection}\label{sec:vertex_selection}
Equation \ref{eq:mgg} demonstrates that the diphoton mass resolution depends not only on the per-photon energy resolutions, but also on the determination of the photon opening angle, $\theta$. For this, it is necessary to know the precise location of the primary hard-scattering vertex from which the photons originate. If the location is correctly determined within 1~cm of the true vertex position, the diphoton mass resolution is dominated by the photon energy resolution. For events with additional objects such as charged leptons and jets, it is relatively easy to assign the primary vertex, due to the presence of distinct charged particle tracks. On the other hand, for \Hgg events with no additional objects (e.g. ggH 0J), it becomes a much more difficult problem. In the \Hgg anaylsis, the diphoton vertex assignment is performed using a dedicated BDT, which is trained using ggH \Hgg simulated events and takes as input variables related to the tracks recoiling against the diphoton system:

\begin{itemize}
    \item $\sum_i |\vec{p}^{\,i}_T|^2$,
    \item $-\sum_i (\vec{p}^{\,i}_T\cdot \vec{p}_T^{\,\gamma\gamma}/|\vec{p}_T^{\,\gamma\gamma}|)$,
    \item $(|\sum_i \vec{p}^{\,i}_T| - p_T^{\,\gamma\gamma})/(|\sum_i \vec{p}^{\,i}_T| + p_T^{\,\gamma\gamma})$,
    \item the total number of photon conversions in the tracker,
    \item the pull ($|z_{\rm{vtx}}-z_{\rm{e}}|/\sigma_z$) between the longitudinal positions of the reconstructed vertex, $z_{\rm{vtx}}$ and the vertex estimated using tracks from photon conversions, $z_e$, where $\sigma_z$ is the uncertainty in $z_{\rm{e}}$,
\end{itemize}

\noindent
where the sum runs over all particle flow tracks associated with a given vertex, labelled by the index, $i$. The quantity $\vec{p}_T^{\,\gamma\gamma}$ corresponds to transverse momentum of the diphoton system, measured with respect to the same vertex. The final two BDT input variables in the list are only included for events which contain tracks originating from photon conversions. 

The performance of the vertex assignment BDT is evaluated using \Zmumu events. Here, the vertices are reconstructed after omitting the muon tracks, in order to imitate a diphoton-like system. The vertex is then said to be correctly assigned by the BDT if the location is within 1~cm of the true vertex position. Figure \ref{fig:vertex_selection_0} (left) shows the efficiency of the vertex assignment as a function of the dimuon $p_T$, for both simulation and data, demonstrating an agreement within 2\% across the whole $p_T$ range. Correction factors are applied to simulation to match the efficiencies observed in data, keeping the total number of events constant. For inclusive \Hgg events, the efficiency of correctly assigning the diphoton vertex to be within 1~cm of the true vertex is roughly 79\%. This value rises for events with additional objects such as jets and charged leptons.

An additional vertex-related BDT is trained to estimate the \textit{probability} that the vertex assignment is within 1~cm of the interaction point from which the diphoton originates. The inputs variables are the total number of reconstructed vertices in an event, the relative positions and respective vertex assignment BDT scores for the three highest scoring vertices, $|{p}_T^{\,\gamma\gamma}|$, as well as the number of converted photons in the tracker. Akin to the vertex-assignment BDT, this vertex-probability BDT is trained using ggH, \Hgg events. The right-hand panel of Figure \ref{fig:vertex_selection_0} demonstrates the agreement between the output probability and the vertex assignment efficiency for simulated \Zmumu events.

\begin{figure}[hptb]
  \centering
  \includegraphics[width=1\textwidth]{Figures/hgg_overview/vertex_assignment_0.pdf}
  \caption[Vertex-assignment and vertex-probability BDT]
  {
    Add caption.
  }
  \label{fig:vertex_selection_0}
\end{figure}

Finally, the width of the $z$ distribution of reconstructed vertices, known as the beamspot width, is measured to be between 3.4--3.6~cm in data. A year-dependent correction is applied to simulated events to ensure the beamspot width matches that observed in data.

\subsection{Reconstruction of other objects}
The analysis categories are defined to target events from different bins of the STXS stage 1.2 framework. In order to define such categories it is necessary to place requirements on additional objects in the event, such as two forward jets in VBF production or two same flavour, opposite charge leptons in ZH leptonic production. The following section briefly describes the reconstruction of these additional objects. \textbf{Do particle flow first!}

\subsubsection{Jets}
Jet definition and energy fractions. B-tagging. Pile-up identification score.

\subsubsection{Electrons}

\subsubsection{Muons}

\subsubsection{Missing transverse momentum}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Event categorisation}\label{sec:event_categorisation}

\subsection{Overview}
After the event reconstruction, a total of 80 event categories are constructed in which the \mgg spectrum is fit simultaneously to measure Higgs boson properties. Each category imposes a set of selection criteria related to the features of both the reconstructed diphoton system and the additional objects in the event. The criteria of a given category are defined to maximise the sensitivity to events from a particular STXS stage 1.2 bin (or group of bins if the statistics are insufficient).

Before applying the dedicated per-category selection criteria, all events are required to have at least two pre-selected photon candidates, satisfying $p_T^{\gamma 1}>m_{\gamma\gamma}/3$ and $p_T^{\gamma 2}>m_{\gamma\gamma}/4$, with a diphoton invariant mass in the range $100<m_{\gamma\gamma}<180$~GeV. The scaled transverse momentum cuts on the photons mitigate distortions at the lower end of the mass spectrum. Furthermore, both photons are required to be in geometrical acceptance: $|\eta|<2.5$, excluding the transition region $1.44<|\eta|<1.57$ between the barrel and endcaps. The categories are then constructed, following the procedure described below.

\begin{enumerate}
    \item \textit{Global categories} are defined to be enriched with events originating from the different Higgs boson production modes. Dedicated selection cuts are defined based on the different event topologies, for example VBF-like events are required to have a dijet invariant mass, $m_{jj}>350$~GeV. To further boost the ratio of signal-to-background events (S/B), multivariate classifiers are trained to isolate the targeted signal process from both SM background processes and the other Higgs boson production modes. At least one of these so-called \textit{background rejection discriminants} are used in each global category. In this analysis, global categories are defined to target the tHq, ttH, VH, VBF and ggH production modes, in a variety of final states.
    
    \item The next step is to divide the per-production mode global categories into \textit{analysis regions} to differentiate between different bins of the STXS. This enables cross sections of the STXS framework to be measured individually. For ggH production, excluding the region with high dijet mass (\mjj~$>350$~GeV), the division is performed using a dedicated multivariate classifier with output classes corresponding to the targeted ggH STXS bins. For the other production modes, the global categories are split according to the reconstructed equivalents of the truth-level variables defining the STXS boundaries e.g. $p_T^{\gamma\gamma}$ for $p_T^H$. These divisions are performed as long as there exists some statistical sensitivity to the split STXS bins. If not, then the targeted group of STXS bins are merged together in the final cross section measurements (see section \ref{sec:results_STXS}). 
    
    \item If the statistics are sufficiently large at this stage, a region can be further sub-divided according to the signal-to-background purity. This is performed by placing boundaries on the relevant background rejection discriminant(s). The position of these boundaries is optimised independently in each region to maximise the sensitivity to the targeted STXS bin(s). The usual metric for this optimisation is the approximate median signficance (AMS),
    \begin{equation}
        \rm{AMS} = \sqrt{2\Big((S+B)\ln(1+S/B)-S\Big)},
    \end{equation}
    \noindent
    where $S$ is the number of signal events from the targeted STXS bin(s) and $B$ the number of background events, both calculated within a $\pm1\sigma_{\rm{eff}}$ window of $m_H$. Here, $\sigma_{\rm{eff}}$ is defined as the shortest interval containing 68\% of the total number of targeted signal events. The background estimate, $B$, is determined by fitting an exponential function to background events, either directly to data sidebands or to the MC simulated background events. In the limit of small $S/B$, the AMS reduces to the familiar $S/\sqrt{S+B}$ formula.
    
    The final analysis categories are referred to as \textit{tags}, which are defined in decreasing order of the expected signal-to-background ratio. For example, the tag with the highest S/B, targeting the ggH 0J \ptH~$<10$~GeV STXS bin, is denoted as 0J $p_T^{\gamma\gamma}<10$~GeV Tag0. The total number of tags is realised by a stopping criterion, where the expected AMS gain by introducing an additional boundary is below some threshold e.g. 5\% improvement.
\end{enumerate}

In this analysis, the data and simulation from all three years are merged together in each category. This provides larger statistics for the training of the numerous multivariate discriminants, and the subsequent optimisation of the selection criteria. In addition, the larger yields reduces the uncertainty in the shape parameters of the per-category background models (see section \ref{sec:bkg_modeling}). The potential gain in splitting each category by year, and thus exploiting the year-dependent mass resolution information in the final fit, was found to be negligible. 

It is possible for any given event to pass the selection criteria for multiple analysis categories. A priority sequence is defined to ensure the categories are truly orthogonal: each event is assigned to at most one category, choosing the category with the highest priority. The sequence is defined to prioritise rarer Higgs boson production modes, by ordering according to the expected number of signal events. In other words, global categories with a lower expected signal yield are given a higher priority.

A summary of all analysis categories is provided in Table \ref{tab:categorisation_overview}, ordered according to decreasing priority in the category sequence. Example Feynman diagrams are shown for the targeted Higgs boson production modes. The background rejection discriminants are listed, as well as the methods for splitting the global categories to target specific bins of the STXS framework. Finally, the number of tags for each targeted bin(s) are shown, defined by placing boundaries on the respective background rejection discriminants. A graphical schema of the analysis event categorisation procedure is provided in Appendix XXX.

The remainder of this chapter provides additional detail concerning the optimisation and validation of the categorisation workflow, and shows the performance in terms of the purity of the final analysis categories. For reference, table \ref{tab:categorisation_discriminants} provides a summary of all the multivariate classifiers used in the categorisation, including the full set of input features, the outputs, and the training samples. When training the various classifiers, the samples are first weighted according to their respective SM cross sections, and are subject to the initial selection cuts of the relevant global category. Often, the weights of each output class (e.g. signal and background) are then equalised to improve the classifier performance.

\begin{table}[htb]
    \caption[Add caption]{Add caption.}
    \label{tab:categorisation_overview}
    % \vspace{.5cm}
    \centering
    \tiny
    \renewcommand{\arraystretch}{1.3}
    \setlength{\tabcolsep}{2pt}
    \input{Tables/hgg_overview/categorisation_overview}
\end{table}

\begin{landscape}
\thispagestyle{empty}
\begin{table}[htb]
    \caption[Add caption]{Add caption. Something about pre-selection. Fill in input features. Top associated background (tt+$\gamma\gamma$, tt+$\gamma$+jet, t+$\gamma$+jet, tt+jets.}
    \label{tab:categorisation_discriminants}
    % \vspace{.5cm}
    \centering
    \scriptsize
    \renewcommand{\arraystretch}{1.5}
    \setlength{\tabcolsep}{3pt}
    \input{Tables/hgg_overview/categorisation_discriminants}
\end{table}
\end{landscape}

\newpage
\subsection{ggH categorisation}
In order to introduce some of the key techniques used in the event categorisation, it is useful to begin at the end of the priority sequence and work backwards. The final global category in the sequence targets the ggH production mode and considers all events that pass preselection but are not selected by higher priority tags. Firstly, the ggH BDT is used to predict the most probable ggH STXS region from which the event originates. The BDT outputs nine class probabilities corresponding to the region with $p_T^H>200$~GeV and the eight STXS bins with $p_T^H<200$~GeV, excluding the VBF-like region with dijet invariant mass, $m_{jj}>350$~GeV. To minimise model-dependence, the BDT is not trained to distinguish between the four bins with $p_T^H>200$~GeV; instead events in this class are further split into the final STXS bins using the reconstructed \ptgg. Events with dijet invariant mass, $m_{jj}>350$~GeV, are not assigned by the ggH BDT and are instead classified using the dijet BDT, as described in section \ref{sec:qqH_categorisation}.

The nine ggH classes are uniquely defined by the Higgs boson transverse momentum and the number of jets. It was found that the ggH BDT outperforms the assignment from using the reconstructed \ptgg and number of jets alone. This improvement arises as the BDT can exploit the correlations between the photon and jet kinematic properties, and thus use the well-measured photon quantities to infer information related to the less well-measured jets. As a result, the correct assignment of the per-event number of jets increases from 77\% to 82\%; the improvement is negligible for the \ptgg assignment since this is already a well-measured quantity with little migration across the \ptH boundaries. Propagating this to the final results leads to an improvement in the measured cross sections, particularly for the 0J and 1J bins, and reduces the correlations between measured quantities.

Each event is assigned to the output class with the highest probability. The final 12 analysis regions (9 output ggH BDT classes, where the $p_T^H>200$~GeV class is further split using boundaries in the reconstructed \ptgg at 300, 450 and 650~GeV) are then divided into the final tags using the diphoton BDT. This BDT is trained to discriminate all Higgs boson signal from all other modes of SM diphoton production, meaning the output score, as shown in Figure \ref{fig:diphoton_score}, can be used to reduce the background contamination in each of the analysis regions. The boundaries on the diphoton BDT output are optimised independently in each region to maximise the expected AMS. Three tags are defined for the eight STXS bins with $p_T^H<200$~GeV, two for the two bins with $200<p_T^H<450$~GeV, and one for the two bins with $p_T^H>450$~GeV. Events that fail the lowest diphoton BDT output threshold in the respective analysis region are discarded from the analysis. 

Table \ref{tab:ggH_category_yields} presents the expected signal and background yields in each of the ggH analysis categories. The signal yields are broken down into the fractional contribution from the target STXS bin(s), as well as the fractional contributions from the different Higgs boson production modes.

\begin{figure}[hptb]
  \centering
  \includegraphics[width=.5\textwidth]{Figures/hgg_overview/DiphotonBDT.pdf}
  \caption[Diphoton BDT output distribution]
  {
    The diphoton BDT score distribution for all events passing preselection and satisfying $100<m_{\gamma\gamma}<180$~GeV. Events from simulation and data are shown by the red band and the black points respectively. Histograms are shown for the different contributions of the simulated background in red. The blue histogram corresponds to simulated Higgs boson signal events, produced via gluon-gluon fusion, multiplied by a factor of 100 to ease the comparison with the background distributions. The shaded grey region represents scores below the lowest diphoton BDT threshold used to define the final analysis tags. The full data set collected in the period 2016-2018 is shown.
  }
  \label{fig:diphoton_score}
\end{figure}

\begin{table}[htb]
    \caption[Expected yields for the ggH production mode categories]{The expected number of Higgs boson events ($m_H$~=~125~GeV) in the analysis categories targeting the ggH production mode. The yield is broken down into the fraction originating from the targeted STXS bin(s), as well as the fractional breakdown into the different Higgs boson production modes. The $\sigma_{\rm{eff}}$, defined as the smallest interval containing 68.3\% of the $m_{\gamma\gamma}$ distribution provides an indication of the mass resolution in each category. The final column, shows the expected ratio of signal to signal-plus-background events (S/S+B) in a $1\pm\sigma_{\rm{eff}}$ window, centred on $m_H$. Here, S is the integrated yield of all Higgs boson production modes. (add column for targeted S/S+B only).}
    \label{tab:ggH_category_yields}
    % \vspace{.5cm}
    \centering
    \scriptsize
    \renewcommand{\arraystretch}{1.3}
    \setlength{\tabcolsep}{2pt}
    \input{Tables/hgg_overview/ggh_yields}
\end{table}

\FloatBarrier

\subsection{EW qqH categorisation}\label{sec:qqH_categorisation}
As described in section \ref{sec:theory_stxs}, the definition of qqH production in the STXS framework includes both the $t$ and $u$-channel contributions (VBF production), and the $s$-channel contribution (VH production in which the vector boson decays hadronically). Characteristics of the dijet system are used to construct separate global categories, effectively distinguishing between the VBF and VH topologies.

Of these two, the global category targeting the VBF-like topology is higher in the priority sequence. The VBF-like STXS bins, defined identically in the qqH and ggH schemes, are specified at truth-level as events containing a dijet system, with a dijet invariant mass, $m_{jj}>350$~GeV. At reconstruction-level, the signal region requires events to have at least two jets, with $p_T^{j}>40~(30)$~GeV for the leading (sub-leading) jet, $|\eta^j|<4.7$, and a reconstructed $m_{jj}>350$~GeV. In addition, all jets are required to pass a threshold on the pile-up identification score and photons are required to have an ID BDT score of greater than -0.2. Note, the definition of this signal region includes contributions from both the VBF and ggH production modes (shown by the Feynman diagrams in the VBF-like row of Table \ref{tab:categorisation_overview}). Events are subsequently classified as originating from either VBF, ggH, or from other SM backgrounds using the dijet BDT, where the outputs of the BDT are per-event probability estimates for each of the three classes. 

The dijet BDT is trained using simulated VBF, ggH and SM diphoton production events. Background events in which at least one of the photons originates from a misreconstructed jet are poorly modelled in simulation. This is a result of the difficult to model quark/gluon-fragmentation processes, in addition to the fact that very few events pass the signal region selection criteria defined above. To improve the prediction of this background contribution when training the dijet BDT, simulated events are replaced with data from a dedicated control region, defined to be enriched with $\gamma$~+~jet and jet~+~jet events. The procedure is as follows:

\begin{itemize}
    \item events are split into the signal and control regions. In the control region, at least one of the photons is required to have a photon ID BDT score less than -0.5. This defines two contributions in the control region: fake-prompt (FP) where one of the photons has a score less than -0.5 and the other greater than -0.2, and fake-fake (FF) where both the photons have a score less than -0.5. The events in the signal region are defined by requiring both photons to have ID BDT scores greater than -0.2. Since the photon ID BDT score is used to define the control region then it is not included as an input feature to the dijet BDT.
    
    \item Events in the control region can have a different normalisation and different kinematic properties to the events in the signal region. To correct this, a fake factor is derived in bins of reconstructed photon $p_T^\gamma$ and $\eta^\gamma$ using simulated events, as the ratio of the expected number of events in the signal region, $N^{\rm{SR}}(p_T^\gamma,\eta^\gamma)$, to the expected number of events in the control region, $N^{\rm{CR}}(p_T^\gamma,\eta^\gamma)$:
    \begin{equation}
        p_{\rm{fake}}(p_T^\gamma,\eta^\gamma) = \Big(\frac{N^{\rm{SR}}(p_T^\gamma,\eta^\gamma)}{N^{\rm{CR}}(p_T^\gamma,\eta^\gamma)}\Big)_{\rm{MC}}.
    \end{equation}
    
    \item As the SM diphoton contribution of the background is predicted directly from simulation, it is necessary to remove this contribution from the control regions to avoid double counting. Again using simulation, the fraction of background events in the control region which originate from a fake photon ($\gamma$~+~jet and jet~+~jet) is calculated in bins of $p_T^{\gamma}$ and $\eta^\gamma$ as,
    \begin{equation}
        p_{\rm{QCD}}(p_T^\gamma,\eta^\gamma) = \Big(\frac{N_{\gamma j}^{\rm{CR}}+N_{jj}^{\rm{CR}}}{N_{\gamma\gamma}^{\rm{CR}}+N_{\gamma j}^{\rm{CR}}+N_{jj}^{\rm{CR}}}\Big)_{\rm{MC}}.
    \end{equation}    
    
    \item The total transfer factor, evaluated for each photon with photon ID BDT score less than -0.5, is defined as,
    \begin{equation}
        f(p_T^\gamma,\eta^\gamma) = p_{\rm{fake}} \times p_{\rm{QCD}}.
    \end{equation}
    These transfer factors are applied as weights to the data events in the control region to estimate the contribution in the signal region, according to the illustration in Figure \ref{fig:dijetbdt_transferfunctions}. The negative sign for events in the FF region originates by considering the contribution which enters the FP region from the FF region when applying the transfer factors. It is this sample of reweighted data events from the control region which is used to train the dijet BDT.
\end{itemize}

\begin{figure}[hptb]
  \centering
  \includegraphics[width=1\textwidth]{Figures/hgg_overview/dijetbdt_illustration.pdf}
  \caption[Illustration of the data-driven background estimate method for the dijet BDT]
  {
    Add caption.
  }
  \label{fig:dijetbdt_transferfunctions}
\end{figure}

The data-driven approach is validated by comparing the dijet BDT input feature distributions from the background prediction (simulated diphoton component plus the data-driven $\gamma$~+~jet and jet~+~jet components) to data from sidebands in the diphoton mass distribution. This sideband data is subject to the usual VBF-like signal region selection, except the diphoton mass is required to be outside of the range $115<m_{\gamma\gamma}<135$~GeV. Figure \ref{fig:dijetbdt_validation} demonstrates good agreement in the $m_{jj}$ and $\Delta\phi_{jj,\gamma\gamma}$ distributions with the data-driven background estimate (middle panel). In addition, the figure highlights the increase in the training statistics with respect to using simulated events only (bottom panel), thus leading to an improved performance of the classifier. 

\begin{figure}[hptb]
  \centering
  \includegraphics[width=.49\textwidth]{Figures/hgg_overview/DijetBDT_input_Mjj.pdf}
%   \hfill
  \includegraphics[width=.49\textwidth]{Figures/hgg_overview/DijetBDT_input_dPhi.pdf}
  \caption[Dijet BDT input features using the data-driven background estimate]
  {
    Add caption.
  }
  \label{fig:dijetbdt_validation}
\end{figure}

Figure \ref{fig:dijetbdt_outputs} shows the two independent output probabilities of the dijet BDT: the VBF probability, $p_{\rm{VBF}}$ (left), and the ggH probability, $p_{\rm{ggH}}$ (right). The background probability is simply realised according to the equation: $p_{\rm{bkg}}=1-p_{\rm{VBF}}-p_{\rm{ggH}}$. 

\begin{figure}[hptb]
  \centering
  \includegraphics[width=.49\textwidth]{Figures/hgg_overview/DijetBDT_VBFprob.pdf}
  \includegraphics[width=.49\textwidth]{Figures/hgg_overview/DijetBDT_ggHprob.pdf}
  \caption[Dijet BDT output probabilities: $p_{\rm{VBF}}$ and $p_{\rm{ggH}}$]
  {
    Add caption. Update ggH probability plot.
  }
  \label{fig:categorisation_vhlep}
\end{figure}

The VBF-like global category is split to target different bins of the STXS framework. For the qqH STXS bins, events are assigned to one of five regions using the reconstructed equivalents of the truth-level variables. Firstly, the qqH BSM region is defined by requiring $p_T^{\gamma\gamma}>200$~GeV. The four remaining VBF-like bins are targeted according to a boundary in the reconstructed $m_{jj}$ at 700~GeV, and a boundary in the transverse momentum of the dijet plus diphoton system, $p_T^{\gamma\gamma jj}$ at 25~GeV. In each region, two tags are constructed by placing optimised boundaries on the dijet BDT output probabilities. This optimisation considers VBF production as signal and groups ggH production with other background events. In this manner, a lower bound is placed on $p_{\rm{VBF}}$, and an upper bound is placed on $p_{\rm{ggH}}$. On top of this, the optimisation procedure includes a threshold on the diphoton BDT score to further reduce the number of background events.

Finally, an additional two tags are constructed to be enriched in events from the four ggH VBF-like bins. Instead, the optimisation here considers ggH as signal, and VBF as background. This amounts to a lower bound on $p_{\rm{ggH}}$ and upper bound on $p_{\rm{VBF}}$, in addition to a lower threshold on the diphoton BDT score. Events with $p_T^{\gamma\gamma}>200$~GeV are rejected, and are instead considered in the ggH categorisation scheme. No further splitting to target the individual ggH VBF-like bins is performed due to the low expected event yield. 

VH hadronic events enrich a single STXS bin (qqH VH-like) in the EW qqH scheme, defined at truth-level with the requirement, $60<m_{jj}<120$~GeV. To target this bin, an orthogonal signal region is defined with a different selection on the dijet system: events are required to have two jets within $|\eta^j|<2.4$ and with $p_T^j>30$~GeV, satisfying the same pileup identification and photon ID BDT score requirements as the VBF-topology signal region, with a reconstructed $m_{jj}$ between 60 and 120~GeV. 

The VH hadronic BDT is then trained to isolate VH signal events, and subsequently reduce the contribution from ggH and other background processes in the signal region. One of the key input features which helps identify dijets consistent with a vector boson decay is the $\cos{\theta^*}$ variable: the cosine of the difference of two angles, namely the diphoton system in the diphoton-dijet centre-of-mass frame, and the diphoton-dijet system in the lab frame. To train the BDT, the VH, ggH, and SM diphoton samples are taken from simulation, whereas the background with jets faking photons is again derived from a data control region, in an analogous manner to that used for the Dijet BDT. Two final analysis categories are defined from the signal region by placing optimised boundaries on both the VH hadronic BDT and diphoton BDT output scores.

Table \ref{tab:qqH_category_yields} provides a summary of the expected signal and background yields in each of the analysis categories described in this section: those targeting the VBF-topology from both the ggH and qqH production modes and the VH hadronic topology.

\begin{table}[htb]
    \caption[Expected yields for the qqH production mode categories]{The expected number of Higgs boson events ($m_H$~=~125~GeV) in the analysis categories targeting the EW qqH production mode, in addition to the two tags targeting ggH production with a VBF-like topology. The yield is broken down into the fraction originating from the targeted STXS bin(s), as well as the fractional breakdown into the different Higgs boson production modes. The $\sigma_{\rm{eff}}$, defined as the smallest interval containing 68.3\% of the $m_{\gamma\gamma}$ distribution provides an indication of the mass resolution in each category. The final column, shows the expected ratio of signal to signal-plus-background events (S/S+B) in a $1\pm\sigma_{\rm{eff}}$ window, centred on $m_H$. Here, S is the integrated yield of all Higgs boson production modes. (add column for targeted S/S+B only).}
    \label{tab:qqH_category_yields}
    % \vspace{.5cm}
    \centering
    \scriptsize
    \renewcommand{\arraystretch}{1.3}
    \setlength{\tabcolsep}{2pt}
    \input{Tables/hgg_overview/qqh_yields}
\end{table}

\FloatBarrier

\subsection{VH leptonic categorisation}
Three global categories are constructed to target Higgs boson production in association with a leptonically-decaying vector boson. These categories are orthogonal by construction due to requiring a different number of isolated charged leptons in the event: two for ZH leptonic, one for WH leptonic and zero for the VH MET category. The ZH leptonic category imposes a requirement on the dilepton invariant mass, $m_{ll}$, to be between 60 and 120~GeV. This ensures the two leptons are consistent with originating from the decay of a Z boson. In the WH leptonic category, additional requirements are placed on the photon ID BDT score to further reject backgrounds from jets faking photons, and on the invariant mass of the reconstructed lepton with each photon, to reduce the contribution from the Drell-Yan background in which an electron has been misidentified as a photon. The VH MET category is constructed to target events from Z($\rightarrow\nu\nu$)H production, and W($\rightarrow\ell\nu)$H production where the charged lepton is not reconstructed in the detector. Here, events are required to have $p_T^{\rm{miss}}>50$~GeV and to satisfy some criteria on the angle between $p_T^{\rm{miss}}$ and the diphoton system. The categories targeting VH leptonic production are placed between the ttH leptonic and ttH hadronic global categories in the priority sequence.

To further distinguish between VH leptonic signal and background events, a BDT is trained in each global category; the details of which are provided in Table \ref{tab:categorisation_discriminants}. Figure \ref{fig:categorisation_vhlep} shows the output scores of the ZH leptonic and VH MET BDTs. Due to the large contamination of the $\gamma$~+~jet events in the VH MET category, a data-driven approach is used to estimate this component of the background. The precise details of this approach are described in more detail in the context of the dijet BDT in the previous section.

The WH leptonic category is sensitive to a reduced set of the STXS stage 1.2 bins. Boundaries are placed on the reconstructed \ptgg at 75 and 150~GeV, to target the equivalent truth-level splittings in the $p_T^V$ variable. Note, the reconstructed \ptgg provides the best handle on the truth-level $p_T^V$, since the presence of a neutrino in the final state means that the vector boson cannot be fully reconstructed. No splitting of the ZH leptonic or the VH MET categories is performed. The final analysis categories are constructed by placing optimised boundaries on the respective BDT output scores (shown for the ZH leptonic and VH MET BDT scores by the dashed vertical lines in Figure \ref{fig:categorisation_vhlep}). In total, two tags are defined for ZH leptonic, five for WH leptonic (two for the $p_T^V<75$~GeV and $75<p_T^V<150$~GeV bins, one for the $p_T^V>150$~GeV bin), and three for VH MET. The expected signal and background yields for the categories targeting VH leptonic production are shown in Table \ref{tab:vhlep_category_yields}.

\begin{figure}[hptb]
  \centering
  \includegraphics[width=.49\textwidth]{Figures/hgg_overview/ZH_LEPTONIC_BDT.pdf}
  \hfill
  \includegraphics[width=.49\textwidth]{Figures/hgg_overview/VH_MET_BDT.pdf}
  \caption[Output scores of two discriminants used for the VH leptonic production mode categories]
  {
    Distributions of the ZH leptonic BDT (left) and VH MET BDT (right) scores, for both data (black points) and simulated signal and background events (histograms). The $\gamma$~+~jet component of the background is derived directly from data for the VH MET BDT. The statistical uncertainty on the data (simulation) is shown by the black error bars (shaded pink band). The vertical dotted lines indicate the position of the BDT boundaries that define the final analysis tags. Events in the regions shaded grey are not considered for the VH leptonic categories. The signal components have been scaled by 50 and 500 for the ZH leptonic and VH MET BDT distributions respectively.
  }
  \label{fig:categorisation_vhlep}
\end{figure}

\begin{table}[htb]
    \caption[Expected yields for the VH leptonic production mode categories]{The expected number of Higgs boson events ($m_H$~=~125~GeV) in the analysis categories targeting the VH leptonic production modes. The yield is broken down into the fraction originating from the targeted STXS bin(s), as well as the fractional breakdown into the different Higgs boson production modes. Here, ggH also includes contributions from the sub-dominant bbH production mode. The $\sigma_{\rm{eff}}$, defined as the smallest interval containing 68.3\% of the $m_{\gamma\gamma}$ distribution provides an indication of the mass resolution in each category. The final column, shows the expected ratio of signal to signal-plus-background events (S/S+B) in a $1\pm\sigma_{\rm{eff}}$ window, centred on $m_H$. Here, S is the integrated yield of all Higgs boson production modes. (add column for targeted S/S+B only).}
    \label{tab:vhlep_category_yields}
    % \vspace{.5cm}
    \centering
    \scriptsize
    \renewcommand{\arraystretch}{1.3}
    \setlength{\tabcolsep}{2pt}
    \input{Tables/hgg_overview/vhlep_yields}
\end{table}

\FloatBarrier

\subsection{Top-associated categorisation}
The two global categories with the highest priority in the sequence are constructed to be enriched with events from the tHq and ttH production modes, where at least one top quark in the event decays leptonically. In both, events are required to have at least one isolated charged lepton, as well as one or more jets. Moreover, the tHq leptonic selection requires an additional jet, tagged as originating from the hadronisation of a b-quark. 

Due to their similar final state topology, a dedicated deep neural network, referred to as the top DNN, is trained to differentiate between the tHq and ttH production modes. Figure \ref{fig:categorisation_top} (left) shows the output score of the top DNN for tHq, ttH and the relevant SM background processes. A requirement is placed on this score in both the tHq leptonic and ttH leptonic categories to minimise the contamination from the opposing production mode. An additional discriminant is then trained in each global category to reduce the contamination from other non-Higgs boson background processes, such as SM diphoton production with additional top quarks. The output score for the so-called ttH leptonic BDT-bkg is presented in Figure \ref{fig:categorisation_top} (right). 

The ttH leptonic global category is split using the reconstructed diphoton transverse momentum, $p_T^{\gamma\gamma}$, with boundaries matching the truth-level $p_T^H$ splittings of the ttH STXS stage 1.2 bins. Boundaries on the ttH leptonic BDT-bkg score are then optimised separately in each split category to maximise the expected sensitivity to each STXS bin: three tags are defined for the two bins with $p_T^H<120$~GeV bin, two tags for the bin with $120<p_T^H<200$~GeV, and one tag for the two bins with $p_T^H>200$~GeV. Due to the low expected tHq signal yield, a single tag is defined for tHq leptonic.

\begin{figure}[hptb]
  \centering
  \includegraphics[width=.49\textwidth]{Figures/hgg_overview/top_DNN.pdf}
  \hfill
  \includegraphics[width=.4\textwidth]{Figures/hgg_overview/ttH_leptonic_BDTbkg.pdf}
  \caption[Output scores of two discriminants used for the top-associated production mode categories]
  {
    Distributions of the top DNN (left) and ttH leptonic BDT-bkg (right) scores, for both data and simulated signal and background events. The data (CHECK!) events are taken from the \mgg sidebands: $[100,120]\cup[130,180]$~GeV. The statistical (statistical~$\oplus$~systematic) component of the uncertainty on the background estimate is shown by the black (red) shaded band. The grey shaded region for top DNN displays the upper threshold for the tHq leptonic category, whereas for the ttH leptonic BDT-bkg, the grey shaded region illustrates the lowest threshold for the ttH leptonic categories. \textbf{Make these plots consistent (hopefully this will be done for the paper)}.
  }
  \label{fig:categorisation_top}
\end{figure}

The ttH hadronic global category targets ttH production in which both top quarks decay hadronically; entering the category sequence after those which target VH leptonic production. Initially, events are required to have zero isolated leptons and three or more jets, where at least one of the jets is b-tagged. Another discriminant, named the ttH hadronic BDT-bkg, is then trained to further suppress the contamination from non-Higgs boson SM backgrounds. A data-driven technique, analogous to that described for the dijet BDT in section \ref{sec:qqH_categorisation}, is used to achieve a better estimate of the $\gamma$~+~jet component of the background. This improves both the description of the input features and provides a greater number of events with which to train the discriminant. Since the photon ID BDT score is an input to the ttH hadronic BDT-bkg, each event in data is given a new score value, randomly drawn from the respective distribution of simulated $\gamma$~+~jet events which pass the full set of selection criteria. Akin to the ttH leptonic categorisation, the ttH hadronic global category is split into five regions according to the reconstructed $p_T^{\gamma\gamma}$, and independently optimised boundaries are placed on the ttH hadronic BDT-bkg output score. Here, four tags are defined for the $120<p_T^H<200$~GeV STXS bin, three for the $p_T^H<60$~GeV, $60<p_T^H<120$~GeV and $200<p_T^H<300$~GeV bins, and two for the $p_T^H>300$~GeV bin.

Table \ref{tab:top_category_yields} presents the expected signal and background yields in each of the analysis categories targeting the top-associated Higgs boson production modes.

\begin{table}[htb]
    \caption[Expected yields for the top-associated production mode categories]{The expected number of Higgs boson events ($m_H$~=~125~GeV) in the analysis categories targeting the top-associated production modes. The yield is broken down into the fraction originating from the targeted STXS bin(s), as well as the fractional breakdown into the different Higgs boson production modes. Here, ggH also includes contributions from the sub-dominant bbH production mode. The $\sigma_{\rm{eff}}$, defined as the smallest interval containing 68.3\% of the $m_{\gamma\gamma}$ distribution provides an indication of the mass resolution in each category. The final column, shows the expected ratio of signal to signal-plus-background events (S/S+B) in a $1\pm\sigma_{\rm{eff}}$ window, centred on $m_H$. Here, S is the integrated yield of all Higgs boson production modes. (add column for targeted S/S+B only).}
    \label{tab:top_category_yields}
    % \vspace{.5cm}
    \centering
    \scriptsize
    \renewcommand{\arraystretch}{1.3}
    \setlength{\tabcolsep}{2pt}
    \input{Tables/hgg_overview/top_yields}
\end{table}

\FloatBarrier

\subsection{Validation}
It is necessary to validate the modelling of the large number of multivariate classifiers used in this analysis. In the context of event categorisation, validation means requiring a good agreement between data and MC simulation in the outputs of the numerous classifiers, particularly in the signal-like regions where events enter the analysis. If the agreement is poor in a number of the input features then the classifier performance can be sub-optimal. However, if this disagreement propagates to the classifier output then it may introduce a systematic uncertainty on the signal events entering the analysis. No such systematic uncertainty enters for the background estimate, as this is modelled directly from data sidebands (see section \ref{sec:bkg_modeling}).

Independent validation samples are used. The examples in Figure \ref{fig:categorisation_validation} show the validation of the ggH BDT, the diphoton BDT, and the VH hadronic BDT, all using \Zee events in simulation and data. The uncertainty band on the simulation demonstrates the addition in quadrature of the statistical and systematic components, where the systematic component includes uncertainties originating from the photon ID BDT, photon energy resolution, and the jet energy scale and resolution (see section \ref{sec:systematics}). The ggH BDT distribution shows the number of events attributed to each output class. It can be seen that there is an excellent agreement between data and simulation, well within uncertainties, particularly in the signal regions which enter the analysis.

\begin{figure}[hptb]
  \centering
  \includegraphics[width=.55\textwidth]{Figures/hgg_overview/ggH_BDT_validation.pdf}
  \includegraphics[width=.47\textwidth]{Figures/hgg_overview/DiphotonBDT_validation.pdf}
  \includegraphics[width=.49\textwidth]{Figures/hgg_overview/VHHadronicBDT_validation.pdf}
  \caption[Validation of the event categorisation classifiers]
  {
    Add caption.
  }
  \label{fig:categorisation_validation}
\end{figure}

% \FloatBarrier
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
\section{Summary}

This chapter has described the selection, reconstruction and subsequent categorisation of p-p collision events recorded by the CMS detector in order to measure cross sections in the Higgs to two photon decay channel. The confusion matrix in Figure \ref{fig:purity_matrix} displays the expected signal composition of the final analysis categories in terms of a set of merged STXS bins. The numbers in the matrix represent the percentage contributions to the total signal yield in a given category arising from each signal process, such that each row sums to 100\%.

The next chapter explains the procedure for fitting the diphoton mass distributions in each analysis event category, in order to extract constraints on the simplified template cross sections and other parameters of interest.

\begin{figure}[hptb]
  \centering
  \includegraphics[width=1\textwidth]{Figures/hgg_overview/purityMatrix_UL_final.pdf}
  \caption[Confusion matrix for the full set of analysis categories]
  {
    Confusion matrix displaying the composition of each analysis category in terms of a reduced set of STXS bins. The colour scale indicates the fractional yield in each category (rows) accounted for by each STXS process (columns). Each row therefore sums to 100\%. Entries with values less than 0.5\% are not shown. Simulated events for each year in the period 2016-2018 are combined with appropriate weights corresponding to their relative integrated luminosity in data. The column labelled as qqH rest includes the contributions from the qqH 0J, qqH 1J, qqH $m_{jj}<60$~GeV and the qqH $120<m_{jj}<350$~GeV STXS bins.
  }
  \label{fig:purity_matrix}
\end{figure}




